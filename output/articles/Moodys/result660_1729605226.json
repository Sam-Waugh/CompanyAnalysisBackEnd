{
    "headline": "Principal component analysis for yield curve modelling",
    "link": "https___www_moodys_com_web_en_us_insights_resource",
    "content": "ENTERPRISE RISK SOLUTIONS\nAUGUST 2014\nRESEARCH REPORT Principal Component Analysis for Yield Curve\nA UGUST 2014 Modelling\nI NSURANCE ERS\nReproduction of out-of-sample yield curves\nD avid Redfern PhD\nD ouglas McLean PhD\nMoody's Analytics Research\nC ontact Us Overview\nA mericas\n+1.212.553.1658 Â» This note describes principal component analysis (PCA) and our method for using it to model\nclientservices@moodys.com yield curve dynamics. This has particular application to risk drivers representing interest rate\nEurope movements in proxy functions, as generated using the B&H Proxy Generator.\n+44.20.7772.5454\nclientservices.emea@moodys.com Â» The theoretical basis of PCA is explained, along with its relation to model reduction. The\nmain calculation methods for obtaining the principal components (PCs) are presented and the role\nAsia (Excluding Japan)\nof transformations, including centering and scaling as well as more specific transformations, is\n+85 2 2916 1121\nclientservices.asia@moodys.com outlined. The key question of how many PCs (or factors) should be retained in the reduced model\nis addressed.\nJapan\n+81 3 5408 4100\nÂ» The inverse problem of determining the PCs magnitudes that best represent a given yield\nclientservices.japan@moodys.com\ncurve is studied in detail and a simple analytical solution is presented.\nÂ» We consider the trade-off between the number of PCs that are retained and the ability to\nreproduce yield curves to a given accuracy. Of particular concern is the reproduction of out-of-\nsample yield curves (i.e. those that were not used in the analysis to derive the PCs) which often\narise during stress testing. This is shown to potentially require many more PCs than we might first\nthink. A simple method for artificially extending the range of yield curves is presented, which\nmakes the PCs more robust to common yield curve perturbations and/or stresses.\nÂ» In summary, it is not always safe to assume that a two-factor interest rate model means that\nonly two PCs are required or that a three-factor interest rate model means that only three PCs are\nrequired. The actual number of PCs required depends on the desired usage of the reduced model\nin terms of what yield curves are to be reproduced. The danger is that two or three PCs are\nassumed and people rely on these being sufficient to accurately model yield curves that were not\nused in the analysis, such as a 100bp parallel stress.ENTERPRISE RISK SOLUTIONS\nTable of Contents\nIntroduction 3\nDocument structure\nPreliminaries 4\nWhat does a principal component look like? 4\nIntuitive interpretation 4\nPCA as a model reduction technique 6\nThe reduced model in perspective 6\nWhat is the population? 7\nHow well does the sample set represent the population? 7\nWhat if the population changes? 8\nThe inverse problem 8\nSteps to generate the principal components 9\nUsing the covariance matrix 9\nUsing singular value decomposition on the raw data matrix 10\nComparison 10\nData transformations 11\nMean centring and scaling 11\nMore general transformations 11\nAn example 12\nHow many PCs to include in a reduced model 13\nScores 14\nAn example 14\nThe inverse problem: Decomposing a yield curve into principal components 15\nAn example 15\nExpanding the universe of yield curves 16\nThe 100 basis point parallel stress 16\nIncorporating stresses into the reduced model 18\nAn example 18\nConclusions 21\nAppendix A Eigenvalues, eigenvectors and the eigenvalue decomposition 22\nAn example 22\nAppendix B Singular value decomposition 24\nAppendix C Link between SVD and eigenvalue decomposition 24\nSVD applied to a real symmetric matrix 25\nFurther Reading 26\n2 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\n1. Introduction\nThis document describes the method of principal component analysis (PCA) and its application to the selection of risk drivers for\ncapital modelling purposes. There are a multitude of other uses which revolve around common themes of dimension reduction,\nsystem simplification, etc. The main idea behind PCA is that a high dimensional system can be approximated to a reasonable\ndegree of accuracy by a system with a smaller number of dimensions by exploiting correlations between the system variables.\nOne of the main applications in finance is to the modelling of yield curve dynamics. The yield curve could be portrayed in a\nnumber of formats and we may apply PCA to any of these formats, but we focus on forward rates here. If we model the yield\ncurve as a series of points representing forward rates at various terms as presented in Figure 1(a), we might have 50 values\ndescribing the yield curve. Typically a yield curve model is developed and calibrated so as to produce realistic yield curves at future\ntimes. As the yield curve evolves over time, each forward rate can change. It is well understood that adjacent points on the yield\ncurve do not move independently. PCA is a method for identifying the dominant ways in which various points on the yield curve\nmove together.\nUsing a structural model for yield curve evolution such as the two factor Black-Karasinski model, LMM or LMM Plus, realistic\nsamples of the yield curve at a particular time horizon (typically one year) can be obtained. By structural, we mean a stochastic\nyield curve model that has a structure of yield curve evolution over time that is defined by stochastic differential equations. As an\nexample, consider the two factor Black-Karasinski model calibrated for the USD economy at the end of December 2012. We use\n50 forward rates with annual tenor to represent the yield curves. The structural model and its calibration define the yield curve\ndynamics and hence the set of possible yield curves that may be produced in a scenario set, such as those shown in Figure 1(b).\nNote that this set of possible yield curves is usually limited by the structure of the model, often restricted to realistic yield curves\nthat obey no arbitrage conditions or the like. Alternatively, the same PCA methods can be applied to a set of historical yield\ncurves.\nPCA allows us to take such a set of yield curves (or related quantity), process them using standard mathematical methods, and\nthen define a reduced form model for the yield curve (or related quantity). Typically, this reduced form model retains only a small\nnumber of principal components (PCs) but can reproduce the vast majority of yield curves that the full structural model could.\nHence this reduced model has many fewer sources of uncertainty (i.e. dimensions) than if the 50 points of the yield curve were\nmodelled independently. Note that PCA and the reduced model itself have no notion of time evolution. The PCs and the output\nof the reduced model represent the same quantity at the data set analysed, being yield curves (or related quantity) in this study.\nFigure 1 Representation of a yield curve as 50 forward rates with 1-year tenor. This is for the USD economy at the end of\nDecember 2012. Subfigure (b) shows 10000 yield curves (with lines joining the dots for clarity) at a projection\nhorizon of , obtained from the two factor Black-Karasinski model.\n(a) ğ‘¡ =1 (b)\n0.05 0.15\n0.04\n0.1\n0.03\n0.02\n0.05\n0.01\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\nFirstly we deal with some preliminary and background details, including an introduction to principal components and the intuition\nbehind the associated reduced model. In Section 3, we summarise the theory behind PCA and the mathematical methods that\n3 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\ncan be employed. Some useful data transformations are discussed in Section 4, and then Section 5 addresses the problem of how\nmany PCs should be retained in a reduced model.\nSection 6 considers the inverse problem in which we are given a specific yield curve and are required to determine the magnitudes\nof the (small number of) principal components that best reproduces the yield curve. This is arguably the most important section\nas it highlights the limitations of trying to reproduce out-of-sample yield curves, which often arise during stress testing.\nFinally, Section 7 considers some approaches that can be used to expand the universe of possible yield curves to achieve better\nreproduction of out-of-sample yield curves.\n2. Preliminaries\nWhat does a principal component look like?\nIn mathematical terms, a principal component (PC) is a vector, one component for each variable. In general these variables can\nrepresent anything, but for the purposes of this document we consider that the variables are the individual forward rates defining a\nyield curve. Figure 2 shows some of the PCs obtained by applying PCA to the set of yield curves shown in Figure 1(b). These PCs\nare represented by 50 points, but they are shown with lines connecting the points. The PCs are ordered so that the first PC is the\nmost important in capturing variability in the yield curves, the second PC is next most important, and so on. When used in\npractice, PCs are scaled by some real number and so the sign of the actual PC is not important.\nFigure 2 Principal components, represented by 50 forward rates of yield curve, but shown with lines connecting the points.\nSubfigure (a) shows the normalised form of the three most significant PCs, whilst subfigure (b) shows these scaled\naccording to their significance (explained in more detail later). The legend shows the proportion of variability\nattributed to each of the PCs.\n(a) Normalised principal components -4 (b) Scaled principal components\nx 10\n0.4 2\n0\n0.2\n-2\n0\n-4\n(1):0.91 (1):0.91\n-0.2\n(2):0.083 -6 (2):0.083\n(3):0.0031 (3):0.0031\n-0.4 -8\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\nIntuitive interpretation\nThe most intuitive way of obtaining PCs is via eigenvalue decomposition of a covariance matrix (described in detail in a later\nsection). This means we are in the world of variance and covariance, which are measures of central tendency. Thus we are really\ntalking about deviation from the mean. Intuitively, PCs represent ways in which the forward rates making up a yield curve can\ndeviate from their mean levels. With reference to the yield curves in Figure 1(b), the scaled PCs portrayed in Figure 2(b) can be\ninterpreted as follows.\nThe first PC (in blue in Figure 2) represents the situation that all forward rates in the yield curve move in the same direction but\npoints around the 10 year term move more than points at the shorter or longer parts of the yield curve. This corresponds to a\n4 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\ngeneral rise (or fall) of all of the forward rates in the yield curve, but in no way can this be called a uniform or parallel shift. The\nimpact of the first PC can be easily observed amongst the yield curves in Figure 1(b).\nThe second PC (in green in Figure 2) represents situations in which the short end of the yield curve moves up at the same time as\nthe long end moves down, or vice versa. This is often described as a tilt in the yield curve, although in practice there is more subtle\ndefinition to the shape. This reflects the particular yield curves that were used for the analysis, as well as the structural model and\ncalibration that were used to create them. The influence of the second PC is much less easy to observe amongst the yield curves\nin Figure 1(b), where it accounts for only about 9% of the variability in the yield curves.\nThe third PC is further interpreted as a higher order buckling in which the short end and long end move up at the same time as a\nregion of medium term rates move down, or vice versa. In this particular example, this type of movement is only responsible for\nabout 0.31% of the variability. This means that it might be appropriate to use two PCs in a reduced model as these should cover\naround 99% of the yield curve variability.\nFigure 3 shows the net effect of the most important PC in explaining shifts of the forward rates in the yield curve about their mean\nvalues. The solid black line is the mean yield curve at the projection horizon. The broken curves result from addition or\nsubtraction of an arbitrary scaling of the PC. That is, the individual forward rates, (for , representing the forward\nrate applying for terms between and years) are: ğ‘¡ =1\nğ¹ğ‘— ğ‘— =0,â€¦,49\nğ‘— ğ‘—+1\nwhere is the mean value of the th forward rate at the ğ¹ ğ‘— =ğœ‡ğ‘— p +ro ğ›¼je1c ğ‘£ti1o,ğ‘—n , horizon, is the th element of the first PC, and is\na real value used to scale the first PC. In vector notation this is:\nğœ‡ğ‘— ğ‘— ğ‘¡=1 ğ‘£1,ğ‘— ğ‘— ğ›¼1\nwhere represents the entire vector of 50 forward rates m ğ¹ak =in ğœ‡g +up\nğ›¼\nth1ğ‘£e 1y ,ield curve, is the entire vector of mean values of the\nforward rates at the projection horizon, and represents the entire vector that is the first PC. In effect this is a very simple\nreduceğ¹d form model for the yield curves at the specific projection horizon. Sinğœ‡ce the first PC is responsible for 91% of the\nvariability of the set ğ‘¡of= yie1ld curves used in the analğ‘£y1 sis, this reduced model can reproduce a good proportion of the yield curves of\nthe full structural model. That is, suitable values for ğ‘¡w=ill 1produce yield curves that reasonably match many (but not all) of the\nyield curves of the full structural model. The appropriate distribution of values of is a byproduct of the PCA method, as will be\nshown later.\nğ›¼1\nğ›¼1\nNote that the universe of possible yield curves is a result of the shape of this first PC as well as the mean yield curve. To expand\nthe universe of possible yield curves we can simply include more PCs, as described next.\nFigure 3 Yield curves produced by a very simple reduced form model composed of just a single PC. The yield curves are\ndefined as a deviation about the mean yield curve, which is the solid black line. The deviation is by the addition of a\nscaled version of the first principal component to produce yield curves like those shown as broken curves.\n0.06\n0.05\n0.04\n0.03\n0.02\n0.01\n0\n0 10 20 30 40 50\nTerm (Y)\n5 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nPCA as a model reduction technique\nIt should be noted that if we are modelling the yield curves as 50 forward rates, then the covariance matrix will be 50x50 and\nthere will be 50 PCs. These are the 50 eigenvectors produced by the eigenvalue decomposition, with the corresponding\neigenvalues indicating the relative significance of the PCs. A model including all of the PCs turns out to be able to reproduce any\nyield curve, realistic or not. This concept is explored in more detail in later sections. In particular, it can perfectly reproduce all of\nthe yield curves on which the PCA analysis was performed. Such a model should be called a full statistical model, and can be\nrepresented as:\nğ‘›\nğ¹ =ğœ‡+ï¿½ğ›¼ğ‘–ğ‘£ğ‘–,\nwhere is the number of forward rates, equal to 50 in the presentğ‘– =ex1ample. This can be expressed in matrix notation as:\nğ‘›\nwhere is a square matrix whoâ€™s columns are the PCs (i.e. the eigenvectors of the covariance matrix), and\nğ¹ =ğœ‡+ğ‘½ğ›¼,\nis a vector of magnitudes of the PCs.\nğ‘½=[ğ‘£1,ğ‘£2,â€¦,ğ‘£ğ‘›] ğ‘›Ã—ğ‘›\nâŠ¤\nğ›¼It h =as\n[\nğ›¼be1e ,ğ›¼n 2m ,â€¦en ,t ğ›¼ioğ‘›n ]ed that the full structural model has a limited universe of yield curves, whereas the full statistical model\npresented here can reproduce any yield curve. This means that the full statistical model is unnecessarily flexible. As alluded to\nearlier, PCA provides a natural way of simplifying the statistical model whilst maintaining its ability to reproduce the majority of\nyield curves that can be produced by the structural model, by simply reducing the number of PCs that are used. This reduced\nmodel includes only the quantity of PCs that are deemed important, as indicated by the corresponding eigenvalues. In particular,\nwe only retain those PCs that are associated with the largest eigenvalues. The reduced model can be expressed as:\nğ‘˜\nğ¹ =ğœ‡+ï¿½ğ›¼ğ‘–ğ‘£ğ‘–,\nwhere is the number of PCs retained. This means that the yield ğ‘–c=u1rves are expressed as a linear combination of the PCs. This\ncan be expressed in matrix notation as:\nğ‘˜ ğ‘˜\n(ğ’Œ)\nwhere is a rectangular matrix whos columns are the PCs corresponding to the largest\nğ¹ =ğœ‡+ğ‘½ ğ›¼,\neigenvalu(ğ’Œe)s, and is a vector of magnitudes of the PCs.\nğ‘½ =[ğ‘£1,ğ‘£2,â€¦,ğ‘£ğ‘˜] ğ‘›Ã—ğ‘˜ ğ‘˜ ğ‘˜\nâŠ¤\nIn a reduced mod ğ›¼el,\n=\nth [e\nğ›¼\nc1o ,ğ›¼rr2e ,s â€¦po ,n ğ›¼dğ‘˜in ]g population or universe of possible yield curves is also reduced. If the number of PCs is large\nenough the majority of the yield curves used in the analysis can be reproduced to a high level of accuracy. In the modelling of\nyield curves it often suffices to retain just two or three PCs, but the appropriate number can be deduced by considering the\neigenvalues. This is explored in more detail in a later section.\nThe PCs in such a reduced model are also sometimes called factors. A reduced model retaining two PCs is then called a two-factor\nmodel, and a reduced model with three PCs is then called a three-factor model, and so on.\nIn a more advanced setting, it may be advantageous to include various data transformations (such as taking logs of the forward\nrates) in the PC analysis. The corresponding reduced model must then incorporate these data transformations. In particular, full\ndetails of the transformations are required in order to properly deal with the inverse problem of decomposing a given yield curve\ninto magnitudes of the PCs. Such transformations are dealt with in a later section.\nThe reduced model in perspective\nIt is vital that the limitations of the reduced model are fully understood. Figure 4 seeks to put the whole of the process of\ndetermining the reduced model into perspective. Often, the sample set that is analysed in PCA is thought of as the start of the\nprocess, and then the reduced model is thought of as producing yield curves that â€œresemble those in the sample setâ€. However, it\nis important to appreciate the bigger picture. More likely is that the aim of the reduced model is to to be able to produce samples\nfrom the population (or universe) from which the sample set was obtained. It is important to appreciate that the population is not\nthe same as the sample set on which the PCA was performed.\n6 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nFigure 4 The entire process of determining the reduced model.\nThe total population from which the\nPopulation sample set is obtained. For the examples\nin this paper this could also be called the\nuniverse of possible yield curves\nSample set\nThe data set on which the PCA is\nperformed. For the examples presented in\nthis paper this is just a set of absolute yield\ncurves in terms of forward rates.\nPCA\nThe full statistical (PCA) model:\nFull statistical model\nğ‘›\n(retaining all PCs)\nğ¹ =ğœ‡+ï¿½ğ›¼ğ‘–ğ‘£ğ‘–\nğ‘–=1\nThe statistical (PCA) model with a reduced\nReduced model number, , of PCs:\n(retaining fewer PCs)\nğ‘˜\nğ‘˜\nğ¹ =ğœ‡+ï¿½ğ›¼ğ‘–ğ‘£ğ‘–\nğ‘–=1\nWHAT IS THE POPULATION?\nThe population governs what can be contained in the sample set. In the case of yield curves, the population may also be called the\nuniverse of possible yield curve and could be:\nÂ» Synthetically modelled. This includes out put from a structural model representing yield curve dynamics, such as the output\nproduced by a particular calibration of a scenario generator. These could be yield curves produced at a given time horizon\n(such as t=1 yield curves), yield curves produced at all time horizons, changes in yield curves over particular time intervals,\netc. It could be a population of yield curves in terms of forward rates, yield curves in terms of spot rates, or any other way.\nMost of the examples presented in this document fall into this category. This type of population is likely to be infinite (or\nexceedingly large), meaning that different random number seeding can be used to produce a different sample sets from the\nsame population. Note that a different calibration of the models would result in a different population.\nÂ» Historically inferred. The population here could be absolute yield curves for any particular historical frequency for a given\neconomy or a given set of economies. It could also be changes in the yield curve over a particular period of time for one or\nmore economies, etc. The population could also be considered to be infinite here, despite the fact that there are only a finite\nand relatively small number of yield curves (or changes in yield curves, etc) that have been observed. It is often forgotten that\nthese observed yield curves are just a sample set from the population of yield curves that could have occurred. Thinking of\nthis population of yield curves that might have occurred is a much more subjective task, but it is one that must be addressed\nif we are to use the reduced model to predict future yield curves in a responsible way.\nHOW WELL DOES THE SAMPLE SET REPRESENT THE POPULATION?\nFor synthetic sample sets this is often thought of as a task of including more and more samples from the structural model. This\nshould reduce sampling error and is usually a sufficient strategy if the population is defined to be the set of yield curves that can\nbe produced by the structural model, given its particular calibration. If the population is further widened to include any realistic\nyield curves then we should be aware of the ways in which the structural model and its calibration may deviate from reality.\nFor historical sample sets, a certain amount of expert judgement should be used when considering just how well the observed set\nof yield curves matches the population of yield curves that could have occurred (or could occur in the future, if this is relevant). A\nnumber of economic and financial theories could come into play, including no arbitrage principals, term premium assumptions,\nyield curve extrapolation, etc.\n7 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nThere are also artificial concepts that might be worth considering such as stress testing mechanisms that regulators or providers of\nbest practice risk management solutions might dictate.\nThere is always likely to be a difference in coverage between the population and the sample set on which the PCA is performed.\nUnderstanding this gap is key to understanding the limitations of what the reduced model can and cannot be used for. This can\nalso be used to inform the construction of the reduced model, concerning the number of PCs that are retained in the reduced\nmodel (considered in more detail in Section 5), or through selection of the sample set itself, which is the subject of Section 7.\nWHAT IF THE POPULATION CHANGES?\nIf the sample set of yield curves is obtained from a structural model then the population of yield curves could easily be changed by\na standard recalibration. If the population of yield curves (or related quantity) does change then, in theory, the PCA should be\nrepeated. A new sample set should be obtained that represents the new population, the PCA calculations should be repeated and\na new reduced model obtained. In practice, the degree to which this is essential will depend on the extent to which the population\nhas changed. It is possible that the same PCs might be appropriate if the change in the population is only minor or if sufficient\nflexibility had already been build into the reduced model. This is discussed further in Section 7.\nThe inverse problem\nMathematical literature often talks of the inverse problem as being the task of reversing the normal calculation steps. That is,\ndetermining the inputs from a given output (or set of outputs). These can be notoriously difficult problems, often due to the\nforward problem not being one-to-one. In some cases, specific values of the output may not be achievable for any combination of\nvalues of the inputs. The problem is then usually considered to be that of determining the values of the inputs that most closely\nproduce the desired output values. In other cases, the outputs may be identically the same (or the same to the level of numerical\nprecision) for more than one set of input values. The problem is then either to choose one such set of input values according to\nsome criteria or to reduce the ambiguity in possible input values as much as possible.\nIn the current setting, the inverse problem can be defined as the task of working backward from a given yield curve to determine\nthe inputs that could be used to create it. In terms of inputs, we can reasonably consider that the model structure (that is, the\nform of the structural model or the number of PCs in the reduced model) and calibration parameters are known and fixed. Hence,\nit is only the random samples in the structural models or the PC magnitudes of the statistical models that are to\ndetermined. Figure 5 shows this schematically.\nThere are a number of things that need to be considered when dealing with the inverse problem. Firstly, we need to understand\nthe universe of yield curves that can possibly be produced by the particular model. Secondly, we need to consider the difficulty (in\nterms of speed and robustness) of any calculation methods. On both of these counts it will be shown that the statistical/reduced\nmodels that make use of PCA have distinct advantages. This is explored in more detail in Sections 6 and 7.\nFigure 5 Schematic of the forward problem and the inverse problem.\nForward problem\nReduced model\nInputs Outputs\n( values) ğ‘˜ (yield curve )\nğ›¼ğ‘– ( ,\nğ¹ a= ndğœ‡ th+\ne\nï¿½\nğ‘–=â€™1s\nğ›¼ ağ‘– reğ‘£ ğ‘– f,\nixed) ğ¹\nğœ‡ ğ‘˜ ğ‘£ğ‘–\nInverse problem\n8 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\n3. Steps to generate the principal components\nThis section describes how to determine a set of PCs that are appropriate to represent a given structural model. This section also\nexplains how to determine the number of PCs that should be retained when constructing a reduced model.\nIn terms of calculations there are two main ways that PCs can be determined: using eigenvalue decomposition; and using singular\nvalue decomposition. In this section we concentrate on the simple case of applying PCA to yield curves that have not been\ntransformed in any way, but mention where extensions can be made. These extensions are considered in more detail in later\nsections.\nUsing the covariance matrix\nThis method requires us to determine a covariance matrix describing the movement in the points in the yield curve and then\nperforming eigenvalue decomposition (diagonalisation) or singular value decomposition of this covariance matrix.\n1. Generate a set of yield curves (in terms of forward rates) from the structural model. Construct a data matrix of forward\nrates:\nwhere is the th forward rate of the th yield curve. Th ğ‘­at\n=\nis,\nï¿½\nğ¹eğ‘–ağ‘—c ï¿½h\n,\nrow of the matrix is a yield curve and each column\nis a set of values of a particular forward rate.\nğ¹ğ‘–ğ‘— ğ‘— ğ‘– ğ‘­\n2. Transform the values in the data matrix as desired (all optional). See later description. Note that the transformations\nhere need to be taken into account when the inverse problem of determining PC magnitudes that best match a yield\ncurve is tackled.\n3. Centre and scale the data matrix if desired:\na. Subtract the mean of each of the forward rates to obtain data that has zero mean.\nb. Subtract the mean and then divide by the standard deviation of the forward rates, to obtain data that has zero\nmean and standard deviation of 1.0.\n4. Calculate a matrix of covariances between the (possibly transformed, centred and/or scaled) forward rates\nwhere this is interpreted as a matrix of covariances of pairs of columns. Note that if the data has mean of zero and\nğ‘ª=ğ¶ğ‘œğ‘£(ğ‘­),\nstandard deviation of 1.0 this will also be the correlation matrix.\n5. Compute the eigenvalue decomposition or singular value decomposition, with eigenvalues or singular values in\ndecreasing order. The eigenvalue decomposition is:\nâŠ¤\nwhere is an orthonormal matrix and is a diagonal matrix. See 0 for more details of the eigenvalue decomposition.\nğ‘ª=ğ‘½ğš²ğ‘½ ,\nThe singular value decomposition is:\nğ‘½ ğš²\nâŠ¤\nwhere is a diagonal matrix and and are orthonormal matrices that will be identical (to machine precision) since the\nğ‘ª=ğ‘¼ğ‘ºğ‘½ ,\ncovariance matrix is positive semidefinite. See Appendix B and Appendix C for more details of the singular value\ndecomğ‘ºposition. ğ‘¼ ğ‘½\nIn both cases, the columns of are normalised eigenvectors . These are the principal components that we seek.\n6. The significance, , of each p ğ‘½rincipal component is given by: ğ‘£ ğ‘–\nğœ“ğ‘–\nğœ†ğ‘– ğ‘ ğ‘–\nğœ“ğ‘– = ğ‘› , or ğœ“ğ‘– = ğ‘› .\nâˆ‘ğ‘—=1ğœ†ğ‘— âˆ‘ğ‘—=1ğ‘ ğ‘—\n9 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nwhere are the diagonal elements of the diagonal matrix , and are the diagonal elements of the diagonal matrix .\nThis provides information as to how many PCs are required in order to capture the range of yield curves that the\nunderlyğœ†inğ‘– g model can produce. See Section 5 for more detğš²ails. ğ‘ ğ‘– ğ‘º\n7. (Optional) Compute the mean of each forward rate. This mean does not affect the covariance matrix since, by definition,\nthe covariance is a measure of the joint deviation from the mean values. Hence it will not affect the PCs that are\nobtained. However, this vector of mean values is needed to be known if the inverse problem is to be solved. That is, if a\nyield curve is to be decomposed into principal components. Any drift governed by the underlying model structure is\ncaptured in this mean, as is the roll forward of the yield curve due to time evolution.\nUsing singular value decomposition on the raw data matrix\nThe method outlined here applies singular value decomposition (SVD) to the raw data value matrix (not a covariance or\ncorrelation matrix). There is no need to explicitly determine the covariance/correlation matrix if this is not otherwise required, but\nthis also means that mean shifting is significant here.\nThe steps to obtain the PCs using singular value decomposition (SVD) are similar to those outlined above.\n1. Follow step 1 from the eigenvalue decomposition approach outlined above.\n2. Follow step 2 from the eigenvalue decomposition approach outlined above.\n3. Centre the data and scale if desired:\na. Subtract the mean of each of the forward rates to obtain data that has zero mean. Note this was not required\nwhen using decomposition of the covariance matrix, but makes a material difference here.\nb. Subtract the mean and then divide by the standard deviation of the forward rates, to obtain data that has zero\nmean and standard deviation of 1.0.\n4. Compute the singular value decomposition on the (possibly transformed, centred and scaled) matrix F:\nâŠ¤\nSee Appendix B for more details of the singular value decomposition. As before, the columns of are the principal\nğ‘­=ğ‘¼ğ‘ºğ‘½\ncomponents in normalised form. Here these are the eigenvectors of . Note that when the columns of have\nmean of zero then and hence the principal components wâŠ¤ill be the same as befoğ‘‰re.\nğ‘£ğ‘– ğ‘­ ğ‘­ ğ‘­\nâŠ¤\n5. The proportion of v ğ‘­aria ğ‘­n =ce ğ¶ex ğ‘œp ğ‘£la (ğ‘­in )ed by each PC, , is given by\nğœ“ğ‘–\nï¿½ğ‘ ğ‘–\nğœ“ğ‘– = ğ‘› .\nThis provides information as to how many PCs are requâˆ‘irğ‘—e=d1 i ï¿½n ğ‘ oğ‘—rder to capture the range of yield curves that the\nunderlying model can produce.\nComparison\nThere are more details on the ins and outs of eigenvalue decomposition and singular value decomposition in 0, Appendix B\nand Appendix C. All approaches will produce the same results in many cases. Using the eigenvalue decomposition is usually the\nmost intuitive of the approaches and is easier computationally, despite the need to compute the covariance matrix. One\nadvantage of SVD is that it is more accurate in difficult numerical cases and is the preferred approach for PCA tools implemented\nin some commercial numerical packages such as matlab.\nThe application of SVD directly to the data matrix, , is computationally the most expensive but it may also be the most accurate.\nThis matrix may have many more rows than columns, since it is common to perform the analysis on 1000 or 10000 or more yield\ncurves, which may each be modelled with around 5ğ‘­0 forward rates. Eigenvalue decomposition performed on a 50x50 covariance\nmatrix is usually much quicker than SVD performed on a 1000x50 matrix.\nIn many cases, the covariance matrix may have a very large condition number, indicating that the matrix is nearly singular. In the\nexamples presented in this document the covariance matrix often had a condition number of . If the covariance matrix is\n18\nâˆ¼10\n10 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\ngoing to end up in this condition, it might be best to use SVD directly on the data matrix, especially if PCs of lesser importance are\nto be obtained with good accuracy.\nAnother point to note is that mean centring should always be performed when using SVD directly on the data matrix , whereas\nthis is not necessary when covariance matrix methods are used.\nğ‘­\n4. Data transformations\nMean centring and scaling\nSome text book descriptions include a mean centring of the data and then a scaling of the data before attempting PCA.\nDepending on the nature of the problem and the way it is to be solved, these may or may not be essential.\nMean centring is literally the subtraction of the average (of each forward rate) from the data values so that the new mean value is\nzero. As mentioned earlier, mean centring is crucial when the SVD approach is used but is not necessary when eigenvalue\ndecomposition is used.\nScaling then involves dividing the data by the standard deviation (of each forward rate) so that the standard deviation of the new\ndata is 1.0. Scaling is particularly useful if the variables have quite different orders of magnitude, although this is not likely to be\nthe case for yield curves. If both centring and scaling are performed then the covariance matrix is the same as the correlation\nmatrix. Note that we then need to reverse these transformations (i.e. scale by standard deviation of the original data and add on\nthe mean of the original data) in order to recreate any of the original yield curves.\nMore general transformations\nSince PCA is a purely statistical technique, you can transform the data in almost any way and still successfully apply the technique.\nThe inverse transformation is required for the inverse problem of determining the magnitudes of PCs that produce the best fit to a\ngiven yield curve. Some transforms may mean that the reduced model allows negative interest rates or other undesirable features.\nLet be a transformation function that can be applied to the forward rates in the data matrix. PCA is then performed on the\ntransformed data and the final equation for the reduced model is as follows:\nğ‘‡(â‹…)\nâˆ’1\nğ¹ =ğ‘‡ ï¿½ğœ‡+ï¿½ğ›¼ğ‘–ğ‘£ğ‘–ï¿½,\nwhere represents the entire vector of transformed data points, is tğ‘–he entire vector of mean values of the transformed data,\nrepresents the entire vector of the th PC, and is an appropriate scaling of the th PC. In matrix notation this is:\nğ¹ ğœ‡ ğ‘£ğ‘–\nğ‘– ğ›¼ğ‘– ğ‘–\nâˆ’1 (ğ’Œ)\nwhere is a rectangular mğ¹atr=ix ğ‘‡whoï¿½sğœ‡ co+luğ‘½mnsğ›¼ aï¿½r,e the PCs corresponding to the largest\neigenvalu(ğ’Œe)s, and is a vector of magnitudes of the PCs.\nğ‘½ =[ğ‘£1,ğ‘£2,â€¦,ğ‘£ğ‘˜] ğ‘›Ã—ğ‘˜ ğ‘˜ ğ‘˜\nâŠ¤\nSome useful trans ğ›¼fo =rm [a ğ›¼t1io ,ğ›¼ns2 ,a â€¦re: , ğ›¼ğ‘˜]\nÂ» General scaling. If the raw data is a set of possible yield curves at some time horizon, it might be meaningful to scale by the\ninitial ( ) yield curve to get relative movements. The PCs would then represent proportional changes in the yield curve\nover the given time interval.\nğ‘¡ =0\nğ¹\nwhere is the vector of forward rates representing the inğ‘‡it(iğ¹al) y=ield\nğ¹\n0c,urve ( ). The reduced model is then:\nğ¹0 ğ‘¡ =0\n(ğ’Œ)\nÂ» Logarithmic transformation. Some structural models ğ¹ fo =r n ğ¹o0m .ï¿½i ğœ‡na +l y ğ‘½ield ğ›¼cu ï¿½rves, such as the two factor Black-Karasinski and\nLMM models, have a lognormal structure in order to prevent negative interest rates. Without special treatment, there is\nnothing preventing some of the forward rates from a reduced model being negative, since the scale parameters can take\nany real value, including dramatically negative values. One common way to prevent negative interest rates being produced\nfrom a reduced model is to use a logarithmic transformation of the raw yield curve data before performing PCA.ğ›¼ The\n11 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\ntransformed data can take any real value, but when transformed back to actual forward rates it is theoretically impossible to\nget negative rates. The transformation function is:\nThis means that the reduced model would be:\nğ‘‡(ğ¹)=ln(F).\n(ğ’Œ)\nClearly, the forward rates that result from this reduced model can never be negative, for any real PCs and for any real values\nğ¹ =expï¿½ğœ‡+ğ‘½ ğ›¼ï¿½\nof .\nÂ» Displaced logarithmic transformation. This is another useful transformation that is specifically suited to the LMM Plus model.\nğ›¼\nThe LMM Plus model features a displacement of the forward rates which has the effect of shifting the yield curve downwards.\nThis means that the resulting forward rates can be as negative as the (positive) displacement parameter, and so a plain log\ntransform will break down. However, the displaced forward rates from the LMM Plus models are lognormally distributed, and\nso the following transform can be used:\nwith associated reduced model:\nğ‘‡(ğ¹)=ln(ğ¹+ğ›¿),\n(ğ’Œ)\nClearly, the forward rates that result from this reduced model can never be more negative than , for any real PCs and for\nğ¹ =âˆ’ğ›¿+expï¿½ğœ‡+ğ‘½ ğ›¼ï¿½.\nany real values of .\nâˆ’ğ›¿\nğ›¼\nAn example\nThe following example illustrates some of the benefits of using transformations. Consider the use of a logarithmic transformation\nand mean centring during the PCA on the same set of yield curves as appear in Figure 1(b). The transformed yield curves are\nshown in Figure 6(a). Note that the general shape of the data is more generic and more symmetric, and hence less resembles the\nlognormal distribution usually associated with the two factor Black-Karasinski model. The three largest associated PCs are shown\nin Figure 6(b), including the details of the proportion of variability attributable to each of the PCs.\nCompared with the non-transformed PCs shown in Figure 2(b), the transformations have shifted some importance from the first\nPC to the second PC. Now only around 71% of the variability can be obtained using a reduced model with only a single PC.\nHowever the important thing here is that the importance of the third PC has decreased from 0.31% to 0.017%. This means that a\ngreater proportion of the variability in the yield curves can be explained by using a reduced model with two PCs, something over\n99.9%.\nFigure 6 The transformed yield curves corresponding to the example using the two factor Black-Karasinski model presented\nin Figure 1(b). The corresponding (scaled) PCs are also shown, along with proportion of variability attributable to\neach PC, which can be directly compared with the results in Figure 2(b).\n(a) The transformed yield curves (b) Scaled principal components\n3 0.8\n2 0.6\n0.4\n1\n0.2\n0\n0\n-1\n-0.2 (1):0.71\n-2 -0.4 (2):0.29\n(3):0.00017\n-3\n0 10 20 30 40 50 0 10 20 30 40\nTerm (Y) Term (Y)\n12 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\n5. How many PCs to include in a reduced model\nThe question of just how many PCs should be retained in a reduced model is common. It has been mentioned before that if we\nretain all of the PCs then we can exactly reproduce all of the yield curves on which the PCA was performed. Note that this set of\nyield curves is only a sample set obtained from the infinite population of yield curves that the structural model can produce. This\nmeans that it is possible that the full structural model can produce a yield curve (that was not in the set of yield curves on which\nPCA was applied) that the reduced model cannot exactly reproduce, even with all of the PCs.\nThe more PCs are retained the more accurately the reduced model will be able to reproduce the yield curves on which the PCA\nwas performed. In some cases it may be important to allow flexibility to be able to fit yield curves that were not in the set of yield\ncurves on which the PCA was performed. This might also include perturbed or stressed yield curves that cannot even be produced\nby the full structural model. Hence we may decide to retain a relatively large number of PCs. However, with more PCs comes\nmore complexity which may cause problems in systems in which the reduced models are to be used. In some cases it is desirable\nto retain the smallest number of PCs that can reasonably approximate most of the yield curves. It is important to be aware of the\ntrade-off between the number of PCs that are retained and the ability to reproduce yield curves to a given accuracy.\nAs mentioned in Section 3, the proportion of variance explained by each principal component is given by:\nğœ†ğ‘– ğ‘ ğ‘–\nğœ“ğ‘– = ğ‘› or ğœ“ğ‘– = ğ‘› ,\nwhen eigenvalue or singular value decomposition isâˆ‘ pğ‘—e=r1foğœ†rğ‘—med on the covarianâˆ‘cğ‘—e= m1ğ‘ ağ‘—trix. Alternatively,\nï¿½ğ‘ ğ‘–\nğœ“ğ‘– = ğ‘› ,\nwhen singular value decomposition is applied to the raw data mâˆ‘atğ‘—r=ix1.\nï¿½\nTğ‘ hğ‘—ese expressions come about because of the properties of\nthe matrix decompositions that are used. In the case of the eigenvalue decomposition approach, the decomposition is performed\non the covariance matrix. Since the eigenvectors are orthogonal and normalised, the eigenvalues are then measures of the\nvariance of the corresponding forward rate in a coordinate system in which the transformed forward rates are independent (See 0\nfor more details of the interpretation of the eigenvalue decomposition). This means that the denominator in the expressions\nabove represents the total variance (i.e. the sum of the variances of independent variables), and then is the proportion of the\ntotal variance attributed to the th PC.\nğœ“ğ‘–\nThe variance of the reduced model as a proportion of the variance of the set of yield curves that were analysed is given by the\nğ‘–\ncumulative sum of the expressions above for each PC in the model. That is:\nğ‘˜\nï¿½ğœ“ğ‘–,\nwhere is the number of PCs included in the reduced model, kğ‘–=ee1ping in mind that the PCs are included in the reduced model in\norder of decreasing eigenvalue (or singular value). That is, PCs associated with the largest eigenvalues are included before PCs\ncorrespğ‘˜onding to smaller eigenvalues. Hence, based on this measure, there is diminishing worth in retaining more and more PCs in\nthe reduced model.\nThe number of PCs in the reduced model should be such that the expression above is close to 1.0, meaning that almost 100% of\nthe variance is covered. In some cases it might be sufficient that 95% of the variance is covered, but in other applications a higher\ndegree of coverage might be required.\nIt is worth remembering that these measures of variability (and hence ) are only with-respect-to the set of yield curves on which\nthe PCA was performed. If this set does not sufficiently represent the population or universe of yield curves of interest (as outlined\nin Section 2), then using the criteria outlined above may not be\nappropğœ“riğ‘–\nate. This is particularly evident when the sample set\nconsists of historically observed yield curves, since the notion of population is a bit more subjective in this case. But it is also an\nissue for sample sets produced from structural models when the population changes or when we wish the reduced model to be\nable to reproduce yield curves that are vastly different from those in the sample set. This is the subject of Section 7.\nAnother way to consider how many PCs should be retained in the reduced model is to consider scores.\n13 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nSCORES\nScores are the magnitudes of the PCs that are required to fit each yield curve in a particular set. Most often this set is the original\nset of yield curves on which the PCA is performed, but the concept can be extended to slightly different sets. For a given set of\nyield curves, the scores can be determined by solving the inverse problem. Since the PCs are orthogonal this is not a difficult task,\nand is described in detail in Section 6.\nThe following example includes some distributions of scores, and the process is further examined in Section 7 using another\nspecific example.\nAn example\nContinuing the example of the yield curves obtained from the two factor Black-Karasinski model, the proportion of the variance\nattributable to each of the first four PCs is shown in Table 1. PCs obtained from ordinary PCA (as portrayed in Figure 2(b)), and PCs\nusing log transformed and mean centred PCA (as portrayed in Figure 6(b)) are included separately. The cumulative proportion of\nvariance for a reduced model containing the specified number of PCs is also shown for the two cases. In both cases, a great\nproportion of the variance is reproducible if two PCs are included in the reduced model. This is expected since the full structural\nmodel is a two factor model. The proportion covered by the log transformed PCs is larger (except for the reduced model with only\na single PC), highlighting the benefits of the log transform in this example. Note that, in terms of variance coverage for this\nparticular set of yield curves, it would even be better to use the log transformed model with two PCs (99.98%) than the non-\ntransformed model with three PCs (99.93%).\n0(a) shows the distribution of scores for the first 15 PCs for the log transformed case. For each PC, the edges of the box signify the\n25th percentile and 75th percentile of the distribution and the red central line is the median. The whiskers extend 1.5 times the\ninterquartile range either side of the box, and outliers (marked with black dots) are any points lying outside the whiskers. This tells\nus that there is significant requirement for the first two PCs and only very tiny need for any other PC. 0(b) shows a histogram of\nthe distribution of magnitudes of the first PC, giving a more complete view of the distributional form.\nAs has been previsouly mentioned, care should be taken when using these criteria to determine the number of PCs to retain since\nthey are only measured against the set of yield curves on which the PCA was performed, rather than the full population or universe\nof yield curves that the full structural model can produce. The indicative number of PCs should only be considered as a minimum.\nTo ensure that the indicated number of PCs here is more meaningful the set of yield curves should be quite large. Additional PCs\ncan be included in the reduce model, in order to build in some flexibility to reproduce a wider universe of yield curves if desired.\nThe ideas behind this concept are extended in the Section 7.\nTable 1 The proportion of the variability that can be explained by the first four PCs, for the example of the yield curves\nobtained from the two factor Black-Karasinski model. PCs obtained from ordinary PCA (as portrayed in Figure\n2(b)), and PCs using log transformed and mean centred PCA (as portrayed in Figure 6(b)) are included.\nOrdinary PCA Log transformed and mean centred PCA\nPC # Proportion of variance Cumulative proportion of Proportion of variance Cumulative proportion of\nattributable to this PC variance including this PC attributable to this PC variance including this PC\n1 91.36% 91.36% 71.26% 71.26%\n2 8.25% 99.61% 28.72% 99.98%\n3 0.31% 99.93% 0.017% 99.998%\n4 0.0659% 99.996% 0.00146% 99.9998%\n14 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nFigure 7 Scores of the log transformed and mean centred PCA for the example of the yield curves obtained from the two\nfactor Black-Karasinski model. Subfigure (a) portrays a box plot showing the distribution of scores of the first 15\nPCs. For each PC, the edges of the box signify the 25th percentile and 75th percentile of the distribution and the red\ncentral line is the median. The whiskers extend 1.5 times the interquartile range either side of the box, and outliers\n(marked with black dots) are any points lying outside the whiskers. Subfigure (b) shows a histogram of the\ndistribution of magnitudes for PC #1.\n6\n4\n2\n0\n-2\n-4\n-6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nPC #\n6. The inverse problem: Decomposing a yield curve into principal components\nThis section deals with the problem of decomposing a given yield curve into an approximate weighted sum of a given set of PCs.\nThis is a fairly common requirement that comes about when a reduced form model is used in practice. For example, when risk\ndrivers for a proxy function are chosen to be magnitudes of a set of PCs, the magnitudes of these PCs need to be determined from\na specific yield curve when the proxy function is to be evaluated. These magnitudes are also known as scores.\nIt was mentioned earlier that the full statistical model with all PCs can reproduce any yield curve. The magnitudes of the PCs that\ncan be used to exactly match a given yield curve can be determined by a simple matrix multiplication. When the model has been\nreduced to only utilise a smaller number of PCs the universe of possible yield curves is also reduced. This means that there are\nsome yield curves that cannot be reproduced exactly. In this case we are really seeking the magnitudes of the retained PCs that\nbest fit the yield curve in some sense. The same matrix multiplication naturally provides the best fit in the transformed domain in\na least squares sense. Note that no numerical optimisation is necessary to find the best fit for any yield curve.\nThe task is to find the vector that solves the equation . This can be obtained analytically as follows:\n(ğ’Œ)\nğ›¼ ğ¹ =ğœ‡+ğ‘½ ğ›¼\n(ğ’Œ)âˆ’1 (ğ’Œ)âŠ¤\nsince the orthogonality property of the eigenvğ›¼al=ue ğ‘½matrix. m(ğ¹eaâˆ’nsğœ‡ t)h=at ğ‘½ .(ğ¹âˆ’ğœ‡),.\n(ğ’Œ)âˆ’1 (ğ’Œ)âŠ¤\nWhen a data transformation is being used the solution becomes:\nğ‘½ =ğ‘½\n(ğ’Œ)âŠ¤\nwhere is to be interpreted as the mean value of the transformed data rather than the raw data.\nğ›¼ =ğ‘½ .(ğ‘‡(ğ¹)âˆ’ğœ‡),\nAn exağœ‡mple\nIn terms of an example, consider the reproduction of yield curves using the reduced model obtained from the 10000 yield curves\nobtained from the two factor Black-Karasinski model. The PCA is used with a log transform and mean centring and so the PCs are\nthose shown in Figure 6(b). Figure 8 shows the resulting fit to some of the yield curves using the reduced model with two PCs.\nthe first and fourth yield curves are shown in Figure 8(a) and are typical of the fit to the majority of the yield curves that were used\nto determine the PCs. However, since the proportion of variance covered by these two PCs is only 99.98% (see Table 1), the fit to\nsome of the more extreme yield curves is not so quite so good. Yield curves 685 and 988 were identified as particularly extreme\n15 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVES\nedutingam\nCP\n700\n600\n500\n400\n300\n200\n100\n0\n-6 -4 -2 0 2 4\nPC magnitude\ntnuoC\n(a) (b)ENTERPRISE RISK SOLUTIONS\nexamples, and their reproduction is shown in Figure 8(b). Note the slight discrepancy occurring between the 5 year and 10 year\npoints for these particular yield curves.\nFigure 8 Reproduction of yield curves using a reduced model with two PCs. The majority of the yield curves are reproduced\nwith very good accuracy, such as yield curves 1 and 4 in subfigure(a). However there are a few of the more extreme\nyield curves in which there is a significant deviation such as yield curves 685 and 988 from the set of 10000 yield\ncurves from the two factor Black-Karasinski model. The PCs use here are those obtained from the log\ntransformation presented in Figure 6(b).\n(a) Some typical yield curves (b) Some extreme yield curves\n0.06\n0.08 (685): actual\n0.05\n(685): fit\n(988): actual\n0.04 0.06\n(988): fit\n0.03\n0.04\n(1): actual\n0.02\n(1): fit\n(4): actual 0.02\n0.01\n(4): fit\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\n7. Expanding the universe of yield curves\nIn this section we consider an example based on a simple yield curve stress that highlights one important aspect of the tradeoff\nbetween the number of PCs in the reduced model and the entire population or universe of yield curves that can be reproduced.\nThe reproduction of out-of-sample yield curves (i.e. those that were not used in the analysis to derive the PCs) is shown to require\nmany more PCs than we might first think. It is then shown that if we can anticipate the stresses that might commonly be applied\nin practice, there is a simple way to obtain PCs that expand the universe of yield curves to include these stresses. This approach is\nshown to reduce the number of PCs required to reproduce these stressed yield curves.\nThe 100 basis point parallel stress\nConsider the problem of reproducing a yield curve that has been stressed by the addition of 100 basis points uniformly across the\nentire term structure. To be specific, we will consider the first yield curve generated by the two factor Black-Karasinski model as\nshown in Figure 8(a). We use the same reduced model with two PCs as was used in the previous section, using the PCs shown\nin Figure 6(b). Note that this stressed yield curve was not in the original set of yield curves that the (transformed) PCA was\nperformed on. In this regard, this stressed yield curve should be considered to be an out-of-sample yield curve.\nFigure 9(a) shows the best fit using the reduced model with two PCs. Clearly this is a very poor reproduction. This comes about\nbecause the stressed yield curve is not in the universe of yield curves that the reduced model with two PCs can reproduce. It is\nthen natural to consider expanding the reduced model to include another PC, in the hope that the expanded universe of yield\ncurves includes one that closely matches the stressed yield curve we are targeting.\nWith three PCs we see a dramatic improvement to the fit, as shown in Figure 9(b), but this would still not reasonably be\nconsidered good enough for many applications. Adding more and more PCs to the reduced model progressively helps the fit, but\nthere are still significant deviations when even eight PCs are used ( Figure 9(c)). A reasonable fit across the entire term structure is\nonly achieved when nine PCs are retained in the reduced model, as shown in Figure 9(d).\nFigure 10(a) shows the magnitudes of the PCs that are required to best fit this stressed yield curve. Figure 10(b) shows the same\ninformation for 100 basis point stresses of each of the 10000 original yield curves. Clearly the majority of these stressed yield\ncurves require nine PCs in the reduced model for good reproduction.\nThe prime reason that as many as nine PCs are required to reproduced the stressed yield curves is that the PCs were obtained via\nan analysis method that had no knowledge of such stresses. That is, these PCs are not fit for the purpose of reproducing yield\n16 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\ncurves stressed in this way. One way to improve the situation might be to provide such information when the PCs are being\nderived.\nFigure 9 Reproduction of yield curves that have had a 100 basis point (additive) parallel stress applied, using a reduced\nmodel with different quantities of PCs. The PCs use here are those obtained from the log transformation presented\nin Figure 6(b).\n0.06 0.06\n0.05 0.05\n0.04 0.04\n0.03 0.03\n0.02 0.02\n(1): original (1): original\n0.01 (1): actual 0.01 (1): actual\n(1): fit (1): fit\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\n0.06 0.06\n0.05 0.05\n0.04 0.04\n0.03 0.03\n0.02 0.02\n(1): original (1): original\n0.01 (1): actual 0.01 (1): actual\n(1): fit (1): fit\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\nFigure 10 The magnitudes of the PCs that are required to best fit 100 basis point uniform stressed to the yield curves studied\nearlier. For the box plot in subfigure (b), the edges of the boxes signify the 25th percentile and 75th percentile of the\ndistribution and the red central line is the median. The whiskers extend 1.5 times the interquartile range either side\nof the box, and outliers (marked with black dots) are any points lying outside the whiskers.\n1.5\n1\n0.5\n0\n-0.5\n0 5 10 15\nPC #\n17 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVES\nedutingam\nCP\n6\n4\n2\n0\n-2\n-4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nPC #\nedutingam\nCP\n(a) 2 PCs (b) 3 PCs\n(c) 8 PCs\n(d) 9 PCs\n(a) yield curve #1 (b) all stressed yield curvesENTERPRISE RISK SOLUTIONS\nIncorporating stresses into the reduced model\nIf the general nature of the perturbations or stresses that might be required in practice can be anticipated, it is possible to include\nthis information into the PCs. Depending on the actual form of the perturbation, this could be done either:\nÂ» treating it as being separate from the PCA, which would usually mean that it is defined in the original yield curve domain; or\nÂ» as a part of the PCA, either in the original yield curve domain or in the transformed domain.\nThe simple case of a parallel stress could be dealt with either as part of the PCA or as being separate from the PCA. Treating it\nseparate from the PCA, a simple additive shift in the reduced model will achieve the desired effect. For example, for the log\ntransformed reduced model, might look like the following:\n(ğ’Œ)\nwhere is a scalar. This would mean that the universe of possible yield curves would be that of the (transformed) reduced model,\nğ¹ =expï¿½ğœ‡+ğ‘½ ğ›¼ï¿½+ğ›¿,\ngoverned by the PCs that it retains, as well as any parallel shift of such yield curves. Determination of magnitudes of the PCs and\nfor a ğ›¿given yield curve might then be more difficult and might best be done via a numerical optimiser. Note also that this\napproach might detract from some of the advantages of the log transformation, such as the assurance that the entire yield curve\ncğ›¿annot go negative.\nIn many cases, stresses might more naturally be incorporated directly into the PCA. This is as simple as applying the anticipated\nstresses to the original yield curves and performing PCA on the expanded set of yield curves. This does not require much more\ncalculation when the eigenvalue decomposition approach is used since the dimensions of the covariance matrix will still just be\n(reflecting the granularity with which we are representing the yield curves). The PCs obtained will be able to reproduce the\noriginal yield curves as well as the stressed yield curves using an optimal number of PCs.\nğ‘›Ã—ğ‘›\nIt turns out that the stresses need only to be defined in quite general terms. For example, the PCs obtained using a notional\nparallel stress of 100 basis points will serve for a reasonably wide range of parallel stress magnitudes.\nWhilst we have only considered a very simple stress here, the technique for expanding the universe of yield curves can be\ngeneralised to more complex perturbations such as â€œshort end down, long end upâ€, â€œshort end up, long end downâ€, etc.\nAn example\nThe original set of yield curves shown in Figure 1(b) have been put together with the set of corresponding yield curves stressed by\n100 basis points to create a super set. PCA has been performed on this super set to obtain new PCs. These new PCs are shown\nin Figure 11 in both normalized and scaled form. Note that these PCs are slightly different from those obtained earlier (see Figure\n6(b)). There are distinct similarities in the first two PCs, but the importance of the third PC has increased. Note that since there\nare now at least three PCs of significance, it is likely that there has been a sacrifice to the fits of the original yield curves. That is, it\nshould be appreciated that two PCs will no longer be sufficient for a good reproduction of the original yield curves.\nReproduction of some of the original yield curves is shown in Figure 12, allowing us to consider the extent to which the new PCs\nhave degraded the fits. As expected, reproduction of the typical yield curves in Figure 12(a) with two PCs is far from satisfactory.\nThe same two typical yield curves are shown with a more reasonable fit using three PCs in Figure 12(b). Hence, we would expect\nthat at least three of these new PCs would be required to fit the more extreme of the original yield curves. The two extreme yield\ncurves examined earlier ( Figure 8(b)) are considered again in Figure 12(c) and (d), along with fits using three and four PCs. The fit\nusing three PCs is not so bad but might be considered unreasonable for some applications. The fit using four PCs is much better\nbut, as in Figure 8(b), there is still a small part of the term structure over which there is a slight discrepancy.\nReproduction of the stressed yield curve considered earlier is examined in Figure 13. The fit using three PCs is not so bad but there\nare still small parts of the term structure over which there is a slight discrepancy. The fit using four PCs is much better. These can\nbe compared with those shown in Figure 9, where it was seen that as many as nine PCs were required.\nWidening the scope to include all of the yield curves in the super set, Figure 14 shows the PC magnitudes that are required to\nreproduce all of the yield curves, original or stressed. Note that there are very few yield curves requiring a significant magnitude of\nthe fourth PC, and even less requiring a fifth. This is a reflection of the proportion of variability explained by the fourth and fifth\nbeing quite insignificant, as shown in Figure 11(b), and in stark contrast to the nine PCs required for the out-of-sample stressed\nyield curve examined in Figure 9 and Figure 10.\n18 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nFigure 11 Principal components obtained from a super set containing the original yield curves as well as a stressed copy of the\nyield curves. The stress applied here is a 100bp (additive) parallel shift.\n(a) Normalised principal components (b) Scaled principal components\n0.6\n0.5\n0.4\n0\n0.2\n0 -0.5 (1):0.76\n(2):0.19\n-0.2 (3):0.042\n-1\n(4):0.00084\n-0.4 (5):0.00011\n-1.5\n0 10 20 30 40 50 0 10 20 30 40\nTerm (Y) Term (Y)\nFigure 12 Reproduction of original yield curves using new PCs obtained from the super set of original and stressed yield\ncurves. These can be compared with those shown in Figure 8.\n(a) Typical yield curves using 2 PCs (b) Typical yield curves using 3 PCs\n0.06\n0.06\n0.05 0.05\n0.04 0.04\n0.03 0.03\n(1): actual (1): actual\n0.02 0.02\n(1): fit (1): fit\n0.01 (4): actual 0.01 (4): actual\n(4): fit (4): fit\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\n(c) The extreme yield curves using 3PCs (d) The extreme yield curves using 4 PCs\n0.08 0.08\n0.06 0.06\n0.04 0.04\n(685): actual (685): actual\n0.02 0.02\n(685): fit (685): fit\n(988): actual (988): actual\n0 0\n(988): fit (988): fit\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\n19 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nFigure 13 Reproduction of a stressed yield curve using new PCs obtained from the super set of original and stressed yield\ncurves. These can be compared with those shown in Figure 9.\n0.06 0.06\n0.05 0.05\n0.04 0.04\n0.03 0.03\n0.02 0.02\n(1): original (1): original\n0.01 (1): stressed 0.01 (1): stressed\n(1): fit (1): fit\n0 0\n0 10 20 30 40 50 0 10 20 30 40 50\nTerm (Y) Term (Y)\nFigure 14 Box plot showing the magnitudes of the new PCs that are required to best fit each yield curve in the superset of\noriginal and stressed yield curves. For each PC, the edges of the box signify the 25th percentile and 75th percentile of\nthe distribution and the red central line is the median. The whiskers extend 1.5 times the interquartile range either\nside of the box, and outliers (marked with black dots) are any points lying outside the whiskers. This can be\ncompared to Figure 10(b).\n5\n0\n-5\n1 2 3 4 5 6 7 8\nPC #\n20 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVES\nedutingam\nCP\n(a) 3 PCs (b) 4PCsENTERPRISE RISK SOLUTIONS\n8. Conclusions\nThis document has presented the basics of PCA as well as some more advanced concepts. PCA has been presented as a tool\nthrough which a full structural model can be approximated by a simpler statistical model referred to as a reduced model. We have\nstrived to be clear about the association between PCs, the associated reduced model, and the universe of yield curves that it can\nreproduce.\nSpecific techniques for PCA have been explored including eigenvalue decomposition and singular value decomposition, as well as\ntransformations that are useful in some cases, such as for preventing negative interest rates.\nThe problem of how to choose an appropriate number of PCs for a reduced model has been specifically addressed and, in the naÃ¯ve\nuse case, it has been shown to depend on the sizes of the associated eigenvalues of the covariance matrix. More correctly it\ndepends on the desired use of the reduced model, meaning the population or universe of yield curves that we wish to be able to\nreproduce.\nOne important conclusion is that there is a direct relationship between the number of PCs retained in the reduced model and the\nquality of yield curve reproduction. But often the whole purpose of using PCA is to reduce the model to be as simple as possible\nand often there are strong disadvantages in having to deal with more PCs. This means there is a tradeoff between the number of\nPCs in the reduced model and the universe of yield curves that can be reproduced.\nAnother important conclusion is that there is a simple analytic expression for solving the inverse problem of determining the PC\nmagnitudes that best represent a given yield curve. This means that numerical optimisation is not required, either for yield curves\nthat were used to derive the PCs or for any out-of-sample yield curve.\nThe reproduction of out-of-sample yield curves (to reasonable accuracy over the entire term structure) can sometimes require\nmany more PCs than yield curves from which the PCs were obtained. The simple case of a 100 basis point parallel (additive) stress\nof a yield curve was examined in detail and shown to require nine PCs for sufficient reproduction, whereas most of the in-sample\nyield curves only required two PCs. A few methods were proposed to lessen the burden of using many more PCs to reproduce\nout-of-sample yield curves. The most general technique involved anticipating the types of out-of-sample stresses/perturbations\nthat might be required, and expanding the set of yield curves on which the PCA was performed to include stressed yield curves. In\nthe case of the 100 basis point additive stress, this approach was shown to reduce the number of PCs required to reproduce the\nnumber of PCs from nine down to just three or four, depending on the accuracy required.\nIn summary, it is not always safe to assume that a two-factor interest rate model means that only two PCs are required or that a\nthree-factor interest rate model means that only three PCs are required. The actual number of PCs required depends on the\ndesired usage of the reduced model in terms of what yield curves can be reproduced. The danger is that two or three PCs are used\nin a model and people rely on these being sufficient to accurately model yield curve stresses, such as a 100bps parallel shift.\n21 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nAppendix A Eigenvalues, eigenvectors and the eigenvalue decomposition\nIf is a square matrix, the eigenvectors of are the non-zero vectors that satisfy:\nğ´ ğ´ ğ‘£\nwhere is a scalar associated with . That is, matrix multiplication of the eigenvector is equivalent to a scalar multiplication, with\nğ´ğ‘£ =ğœ†ğ‘£,\nthe scale factor being called the eigenvalue. An eigenvector corresponds to a particular eigenvalue, and the pair are specific to the\nmatrix ğœ†. If is a x matrix thenğ‘£ it has eigenvectors (and corresponding eigenvalues) and these eigenvectors are\northonormal, meaning that they are orthogonal and have unit length. In the special case in which the matrix is real and\nsymmeğ´tric, tğ´he eigeğ‘›nvağ‘›lues are all real. ğ‘›\nğ´\nAugmenting the eigenvectors into a column matrix, , and putting the corresponding eigenvalues on the diagonal of a diagonal\nmatrix we get the following matrix equation:\nğ‘› ğ‘‰\nÎ›\nThe orthogonality of the eigenvectors means that and hence . When is invertible this leads to the\nğ´ğ‘‰ =ğ‘‰Î›\neigenvalue decomposition: âŠ¤ âŠ¤ âˆ’1 âŠ¤\nğ‘‰ ğ‘‰ =ğ‘‰ğ‘‰ =ğ¼ ğ‘‰ =ğ‘‰ ğ‘‰\nğ‘‡\nThis can be expanded to express the matrix as a summation as follows:\nğ´=ğ‘‰Î›ğ‘‰ .\nğ´\nğ‘›\nâŠ¤\nğ´=ï¿½ğœ†ğ‘–ğ‘£ğ‘–ğ‘£ğ‘–\nThis summation proves to be particularly useful representationğ‘– =in1 the context of PCA as it clearly indicates that the effect of\neigenvectors diminishes with the size of the eigenvalue. The basis of PCA is that the matrix can be approximated using only\nthose eigenvectors corresponding to significantly large eigenvalues. In some situations this can mean that just a small number of\neigenvectors can be used to represent a system with many tens or hundreds of dimensions. ğ´\nIn terms of interpretation, the eigenvector matrix represents a transformation onto orthogonal dimensions that explain\nmaximum variability. That is, the eigenvector, , corresponding to the largest eigenvalue, , will point in the direction of\ngreatest variability. The second eigenvector, , wğ‘‰ill point in the direction of greatest variability subject to the condition that it\nmust be orthogonal to the . The third\neigenvğ‘£e1\nctor, , will point in the direction of\ngreateğœ†s1\nt variability subject to being\northogonal to both and , and so on. Thiğ‘£s 2 is explained in more detail in the simple example below.\nğ‘£1 ğ‘£3\nThis transformation ğ‘£(c1hange\nğ‘£\n2of basis, or change of coordinates) so that all the points are represented as a linear combination of\nthe eigenvectors , and can be performed by (left) multiplying the data points by . That is, if an existing point is\nthen its representation in the new coordinate system is . ThâŠ¤e values in this new coordinate system\nare the scores. Thğ‘£a1 tâŠ¤ iğ‘£s2 , they ğ‘£ar3 e the magnitudes of the eigenvectors that reproduce tâŠ¤he orğ‘‰iginal data.\nğ‘¥ =[ğ‘¥1,ğ‘¥2,â€¦,ğ‘¥ğ‘›] ğ‘¥Ì… =ğ‘‰ ğ‘¥\nIn matrix form this is\nâŠ¤\nThe elements of (in the new coordinate system) are indepe ğ‘‹ï¿½nd =en ğ‘‰t. T ğ‘‹he covariance matrix is and hence is diagonal. This means\nthat all covariances are zero and the variance of each variable is . The total variance is the sum of the eigenvalues and this is also\nthe total varianceğ‘¥ Ì…of the original data. Î›\nğœ†ğ‘–\nAn example\nConsider the simple example of three variables with normally distributed random samples that are correlated according to the\nfollowing correlation matrix:\n1 0.7 0.3\nğ¶ =ï¿½0.7 1 0 ï¿½.\nThe correlation of the variables means that there will be greater variability in some three-dimensional directions, but these\n0.3 0 1\ndimensions will not generally align with the axes or other convenient directions.\nThe eigenvalue decomposition of gives the following (to three decimal places):\nğ¶\n22 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\n0.707 0 âˆ’0.707 1.762 0 0\nğ‘‰ =[ğ‘£1 ğ‘£2 ğ‘£3]=ï¿½0.650 âˆ’0.394 0.650 ï¿½ ğ‘ğ‘›ğ‘‘ Î›=ï¿½ 0 1 0 ï¿½.\nScatter plots of the points are shown in Figure 15 in the original coordinate system. That is, using regular Cartesian coordinates as\n0.279 0.919 0.279 0 0 0.238\nthe basis: , and . Each point is represented as a linear combination of these orthonormal basis vectors.\nAlso shown in FigâŠ¤ure 15 (in âŠ¤red) are the eâŠ¤igenvectors scaled by the square root of the corresponding eigenvalues. The largest of\nthese eige[n1v,e0c,t0o]rs p[0oi,n1t,0s ]in the d[i0re,0ct,1io]n of maximum variability, which is the eigenvector corresponding to the largest\neigenvalue. This is the direction . This can be seen as the longest red line in the scatter plot of\nvariable 1 versus variable 2, as these are the most highly correâŠ¤lated pair of variables. This same direction (as well as the other\ndirections) is less obvious in the oğ‘£t1 he=r s[u0b.7fi0g7ur,e0s. 6a5s 0th,e0 .e2i7ge9n]vectors do not align nicely with the axes.\nApplying a change of basis (change of coordinate system) so that all the points are represented as a linear combination of the\neigenvectors , and can be performed by (left) multiplying the data points by . The points in the new coordinate system\nare plotted in 0. This gives us a clearer view of the system as the direction of greatest vâŠ¤ariability is (by definition) aligned with the\naxes. The eigğ‘£en1 vğ‘£ec2 tors o ğ‘£f 3 the data points in the new coordinate system are also showğ‘‰n in 0 in red. The eigenvector corresponding\nto the largest eigenvalue is in the direction of the â€œEigenvector 1â€ axis. This gives the direction of greatest variability. The direction\nof the eigenvector corresponding to the second largest eigenvalue points in the direction of greatest variability subject to the\nrestriction that it must be orthogonal to . This is the axis denoted â€œEigenvector 2â€. Finally, the direction corresponding to the\nlast eigenvector is completely defined by the need to be orthogonal to both of the other eigenvectors.\nğ‘£1\nFigure 15 Scatter plots in terms of the points in axes of the original variables. Also shown in red is indication of the\neigenvectors for the covariance matrix of the points in this coordinate system, which are scaled by the square root\nof the corresponding eigenvalue. Note that the eigenvectors point in the direction of largest variability. This can be\nobserved in the scatter plot between variables 1 and 2 (as these are the most highly correlated), but is less clear in\nthe other subfigures because of the three-dimensional nature of the system.\n23 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nFigure 16 The same points as in Figure 15, but transformed into coordinates of the eigenvectors of the covariance matrix of\nthe original points. In this coordinate system the points represent the magnitudes of the eigenvalues that are\nrequired to reproduce the original points. These are sometimes known as the â€œscoresâ€. Also shown in red is an\nindication of the eigenvectors for the covariance matrix of the points in this new coordinate system, which are\nscaled by the square root of the corresponding eigenvalue. Note that these eigenvectors always point in the\ndirection (positive or negative) of the axes, confirming that these new axes are independent.\nAppendix B Singular value decomposition\nSVD reduces a real matrix to three components as follows:\nğ‘šÃ—ğ‘› ğ‘€\nâŠ¤\nHere, is an orthonormal matrix, is an diagonal matrix containing the â€œsingular valuesâ€, and is an\nğ‘€ =ğ‘ˆğ‘†ğ‘‰ .\northonormal matrix.\nğ‘ˆ ğ‘šÃ—ğ‘› ğ‘† ğ‘›Ã—ğ‘› ğ‘‰ ğ‘›Ã—ğ‘›\nThe columns of the matrix are eigenvalues of the matrix , and the columns of the matrix are eigenvectors of the matrix\n. The singular values (diagonal entries of the matrix ) are âŠ¤all non-negative, being the positive square root of the eigenvalues\nof âŠ¤the matrix . ğ‘ˆ ğ‘€ğ‘€ ğ‘‰\nğ‘€ ğ‘€ ğ‘†\nâŠ¤\nThe matrix produced by the multiplication is known as the scores matrix and represents the magnitudes of the columns of\nğ‘€ ğ‘€\nthat produce the raw data in .\nğ‘ˆğ‘† ğ‘‰\nIn theory, SVD is a very stable decomposition as it is always perfectly conditioned. Hence it is often used instead of eigenvalue\nğ‘€\ndecomposition for ill conditioned or rank deficient matrices. Note that correlation matrices can be very highly ill-conditioned.\nAppendix C Link between SVD and eigenvalue decomposition\nIf the original data matrix, , has mean zero, then the pairwise covariance matrix, , is\nğ‘€ ğ¶\n1 âŠ¤\nwhich is a symmetric matrix. Using the SVD of ğ¶ w=e have ğ‘€ ğ‘€,\nğ‘›âˆ’1\nğ‘›Ã—ğ‘› ğ‘€\nâŠ¤ âŠ¤ âŠ¤ âŠ¤ âŠ¤ âŠ¤ âŠ¤ 2 âŠ¤\nand hence\nğ‘€ ğ‘€ =(ğ‘ˆğ‘†ğ‘‰ ) (ğ‘ˆğ‘†ğ‘‰ )=ğ‘‰ğ‘† ğ‘ˆ ğ‘ˆğ‘†ğ‘‰ =ğ‘‰ğ‘† ğ‘‰ ,\n1 2 âŠ¤\nğ¶ = ğ‘‰ğ‘† ğ‘‰ .\nğ‘›âˆ’1\n24 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nDirectly applying eigenvalue decomposition to the correlation matrix gives\nâŠ¤\nEquating these two decompositions of means that:\nğ¶ =ğ‘‰Î›ğ‘‰ .\nğ¶\n1 2 âŠ¤ âŠ¤\nSince the matrix is the same in each case, we obtain an eqğ‘‰uğ‘†atiğ‘‰on l=inkğ‘‰inÎ›gğ‘‰ the. eigenvalues and singular values:\nğ‘›âˆ’1\nğ‘‰\n1 2\nAs both and are diagonal, this equation also holds elemÎ›e=ntwise. ğ‘† .\nğ‘›âˆ’1\nÎ› ğ‘†\nApplication to a real symmetric matrix\nNow consider the application of SVD to a real symmetric matrix, , such as a covariance or correlation matrix. This means that\nğ´\nâŠ¤\nSince is symmetric, then must also be symmetric. This means that , meaning that the elements can only\nğ´=ğ‘ˆğ‘†ğ‘‰ .\ndiffer by a sign change. Relating âŠ¤this to the eigenvalue decomposition, this also implies that the eigenvalues and singular values\ncan alsğ´o only differ by a signğ‘ˆ cğ‘†hğ‘‰ange. It turns out that the differences in sign |ağ‘ˆreğ‘–ğ‘— a|s=so|cğ‘‰iağ‘–ğ‘— te|d with negative eigenvalues and so\nwhenever is positive semidefinite. In this case there is no difference between SVD and eigenvalue decomposition apart\nfrom arbitrary sign changes in the eigenvectors, and numerical accuracy effects when the matrix is ill conditioned.\nğ‘ˆ=ğ‘‰ ğ´\nFor any singular value corresponding to a negative eigenvalue, the corresponding column of is multiplied through by -1. This is\nbasically moving the negative from the eigenvalue to the corresponding column of , so as to make the singular value positive.\nwhen SVD is used, negative eigenvalues can be detected by looking for differences in sign beğ‘ˆtween columns of and .\nğ‘ˆ\nFor the purposes of PCA either method can be used. They both produce the same PCs (eigenvectors) up to a sign change, and\nğ‘ˆ ğ‘‰\nthey both produce the same eigenvalues/singular values up to a sign change.\n25 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nAcknowledgements\nThe authors wish to acknowledge the helpful comments and suggestions of Moodyâ€™s Analytics colleagues Sandy Sharp, Brian\nRobinson, Nick Jessop, and Harry Hibbert.\nFurther Reading\nFor additional details, please read the following resources.\nÂ» Press, Teukolsky, Vetterling and Flannery: \"Numerical Recipes in Fortran 77, The Art of Scientific Computing\" (2nd ed.)\nCambridge University Press, 1992.\n26 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVESENTERPRISE RISK SOLUTIONS\nÂ© 2014 Moodyâ€™s Corporation, Moodyâ€™s Investors Service, Inc., Moodyâ€™s Analytics, Inc. and/or their licensors and affiliates (collectively, â€œMOODYâ€™Sâ€). All rights reserved.\nCREDIT RATINGS ISSUED BY MOODY'S INVESTORS SERVICE, INC. (â€œMISâ€) AND ITS AFFILIATES ARE MOODYâ€™S CURRENT OPINIONS OF THE RELATIVE FUTURE CREDIT RISK OF\nENTITIES, CREDIT COMMITMENTS, OR DEBT OR DEBT-LIKE SECURITIES, AND CREDIT RATINGS AND RESEARCH PUBLICATIONS PUBLISHED BY MOODYâ€™S (â€œMOODYâ€™S\nPUBLICATIONSâ€) MAY INCLUDE MOODYâ€™S CURRENT OPINIONS OF THE RELATIVE FUTURE CREDIT RISK OF ENTITIES, CREDIT COMMITMENTS, OR DEBT OR DEBT-LIKE\nSECURITIES. MOODYâ€™S DEFINES CREDIT RISK AS THE RISK THAT AN ENTITY MAY NOT MEET ITS CONTRACTUAL, FINANCIAL OBLIGATIONS AS THEY COME DUE AND ANY\nESTIMATED FINANCIAL LOSS IN THE EVENT OF DEFAULT. CREDIT RATINGS DO NOT ADDRESS ANY OTHER RISK, INCLUDING BUT NOT LIMITED TO: LIQUIDITY RISK, MARKET\nVALUE RISK, OR PRICE VOLATILITY. CREDIT RATINGS AND MOODYâ€™S OPINIONS INCLUDED IN MOODYâ€™S PUBLICATIONS ARE NOT STATEMENTS OF CURRENT OR HISTORICAL\nFACT. MOODYâ€™S PUBLICATIONS MAY ALSO INCLUDE QUANTITATIVE MODEL-BASED ESTIMATES OF CREDIT RISK AND RELATED OPINIONS OR COMMENTARY PUBLISHED BY\nMOODYâ€™S ANALYTICS, INC. CREDIT RATINGS AND MOODYâ€™S PUBLICATIONS DO NOT CONSTITUTE OR PROVIDE INVESTMENT OR FINANCIAL ADVICE, AND CREDIT RATINGS\nAND MOODYâ€™S PUBLICATIONS ARE NOT AND DO NOT PROVIDE RECOMMENDATIONS TO PURCHASE, SELL, OR HOLD PARTICULAR SECURITIES. NEITHER CREDIT RATINGS\nNOR MOODYâ€™S PUBLICATIONS COMMENT ON THE SUITABILITY OF AN INVESTMENT FOR ANY PARTICULAR INVESTOR. MOODYâ€™S ISSUES ITS CREDIT RATINGS AND\nPUBLISHES MOODYâ€™S PUBLICATIONS WITH THE EXPECTATION AND UNDERSTANDING THAT EACH INVESTOR WILL, WITH DUE CARE, MAKE ITS OWN STUDY AND\nEVALUATION OF EACH SECURITY THAT IS UNDER CONSIDERATION FOR PURCHASE, HOLDING, OR SALE.\nMOODYâ€™S CREDIT RATINGS AND MOODYâ€™S PUBLICATIONS ARE NOT INTENDED FOR USE BY RETAIL INVESTORS AND IT WOULD BE RECKLESS FOR RETAIL INVESTORS TO\nCONSIDER MOODYâ€™S CREDIT RATINGS OR MOODYâ€™S PUBLICATIONS IN MAKING ANY INVESTMENT DECISION. IF IN DOUBT YOU SHOULD CONTACT YOUR FINANCIAL OR\nOTHER PROFESSIONAL ADVISER.\nALL INFORMATION CONTAINED HEREIN IS PROTECTED BY LAW, INCLUDING BUT NOT LIMITED TO, COPYRIGHT LAW, AND NONE OF SUCH INFORMATION MAY BE COPIED\nOR OTHERWISE REPRODUCED, REPACKAGED, FURTHER TRANSMITTED, TRANSFERRED, DISSEMINATED, REDISTRIBUTED OR RESOLD, OR STORED FOR SUBSEQUENT USE FOR\nANY SUCH PURPOSE, IN WHOLE OR IN PART, IN ANY FORM OR MANNER OR BY ANY MEANS WHATSOEVER, BY ANY PERSON WITHOUT MOODYâ€™S PRIOR WRITTEN\nCONSENT.\nAll information contained herein is obtained by MOODYâ€™S from sources believed by it to be accurate and reliable. Because of the possibility of human or mechanical error as well as\nother factors, however, all information contained herein is provided â€œAS ISâ€ without warranty of any kind. MOODY'S adopts all necessary measures so that the information it uses in\nassigning a credit rating is of sufficient quality and from sources MOODY'S considers to be reliable including, when appropriate, independent third-party sources. However,\nMOODYâ€™S is not an auditor and cannot in every instance independently verify or validate information received in the rating process or in preparing the Moodyâ€™s Publications.\nTo the extent permitted by law, MOODYâ€™S and its directors, officers, employees, agents, representatives, licensors and suppliers disclaim liability to any person or entity for any\nindirect, special, consequential, or incidental losses or damages whatsoever arising from or in connection with the information contained herein or the use of or inability to use any\nsuch information, even if MOODYâ€™S or any of its directors, officers, employees, agents, representatives, licensors or suppliers is advised in advance of the possibility of such losses or\ndamages, including but not limited to: (a) any loss of present or prospective profits or (b) any loss or damage arising where the relevant financial instrument is not the subject of a\nparticular credit rating assigned by MOODYâ€™S.\nTo the extent permitted by law, MOODYâ€™S and its directors, officers, employees, agents, representatives, licensors and suppliers disclaim liability for any direct or compensatory\nlosses or damages caused to any person or entity, including but not limited to by any negligence (but excluding fraud, willful misconduct or any other type of liability that, for the\navoidance of doubt, by law cannot be excluded) on the part of, or any contingency within or beyond the control of, MOODYâ€™S or any of its directors, officers, employees, agents,\nrepresentatives, licensors or suppliers, arising from or in connection with the information contained herein or the use of or inability to use any such information.\nNO WARRANTY, EXPRESS OR IMPLIED, AS TO THE ACCURACY, TIMELINESS, COMPLETENESS, MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OF ANY SUCH\nRATING OR OTHER OPINION OR INFORMATION IS GIVEN OR MADE BY MOODYâ€™S IN ANY FORM OR MANNER WHATSOEVER.\nMIS, a wholly-owned credit rating agency subsidiary of Moodyâ€™s Corporation (â€œMCOâ€), hereby discloses that most issuers of debt securities (including corporate and municipal\nbonds, debentures, notes and commercial paper) and preferred stock rated by MIS have, prior to assignment of any rating, agreed to pay to MIS for appraisal and rating services\nrendered by it fees ranging from $1,500 to approximately $2,500,000. MCO and MIS also maintain policies and procedures to address the independence of MISâ€™s ratings and rating\nprocesses. Information regarding certain affiliations that may exist between directors of MCO and rated entities, and between entities who hold ratings from MIS and have also\npublicly reported to the SEC an ownership interest in MCO of more than 5%, is posted annually at www.moodys.com under the heading â€œShareholder Relations â€” Corporate\nGovernance â€” Director and Shareholder Affiliation Policy.â€\nFor Australia only: Any publication into Australia of this document is pursuant to the Australian Financial Services License of MOODYâ€™S affiliate, Moodyâ€™s Investors Service Pty\nLimited ABN 61 003 399 657AFSL 336969 and/or Moodyâ€™s Analytics Australia Pty Ltd ABN 94 105 136 972 AFSL 383569 (as applicable). This document is intended to be provided\nonly to â€œwholesale clientsâ€ within the meaning of section 761G of the Corporations Act 2001. By continuing to access this document from within Australia, you represent to\nMOODYâ€™S that you are, or are accessing the document as a representative of, a â€œwholesale clientâ€ and that neither you nor the entity you represent will directly or indirectly\ndisseminate this document or its contents to â€œretail clientsâ€ within the meaning of section 761G of the Corporations Act 2001. MOODYâ€™S credit rating is an opinion as to the\ncreditworthiness of a debt obligation of the issuer, not on the equity securities of the issuer or any form of security that is available to retail clients. It would be dangerous for â€œretail\nclientsâ€ to make any investment decision based on MOODYâ€™S credit rating. If in doubt you should contact your financial or other professional adviser.\n27 AUGUST 2014 PRINCIPAL COMPONENT ANALYSIS FOR YIELD CURVE MODELLING : REPRODUCTION OF OUT-OF-SAMPLE-YIELD CURVES"
}