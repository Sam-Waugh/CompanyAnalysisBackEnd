{
    "headline": "Economic capital model validation: A comparative study",
    "link": "https___www_moodys_com_web_en_us_insights_resource",
    "content": "Evaluating economic capital models for credit risk is important for both financial institutions and regulators.\nHowever, a major impediment to model validation remains limited data in the time series due to the following\nissues. First, defaults are rare events. One can only observe a limited number of events where extreme losses were\nrealized; second, credit risk models typically estimate value distribution over a one-year horizon, and the number of\nnon-overlapping years is limited in availability. This problem differs from the analysis of market risk, which focuses\non a much shorter horizon.\nThe data constraint makes it difficult to transfer the methodology applied to back-testing market risk models to\ncredit risk models, which require a very long time period to produce sufficient observations for reasonable tests of\nforecast accuracy. To overcome the data limitations, Lopez and Saidenberg (2000) suggested cross-sectional\nresampling techniques, which resample from the original panel dataset of credits to generate additional credit\ndefault/loss data for model evaluation. Frerichs and Löffler (2002) suggested a Berkowitz (2001) procedure to detect\nmis-specified parameters in asset value models, focusing on asset correlations. The authors conducted Monte Carlo\nsimulations to show that a loss history of ten years can be sufficient to detect mis-specified asset correlation in a\ntwo-state credit risk model, but, when applying the test to a multi-state credit risk model, the authors found\n“incorporating migration and recovery rate uncertainty reduces the test’s power.”1 BCBS (2009) interpreted\nvalidation more expansively as an evaluation of all the processes that provide evidence-based assessment regarding\nan EC model's fitness for purpose. The types of validation processes are qualitative processes (e.g., use test,\nmanagement oversight and data quality checks, etc.) and quantitative processes such as validation of inputs and\nparameters, benchmarking, back testing, etc. Following these supervisory standards, Jacobs (2010) surveyed the\ndifferent existing EC model validation practices in the banking industry and illustrated several quantitative\napproaches (benchmarking, sensitivity analysis, and testing for predictive accuracy) by presenting results of a bank\nrisk aggregation study from Inanoglu, et al (2010).\nIn general, there has been limited research that either empirically evaluates credit portfolio risk models or that\ntheoretically develops statistical evaluation methods. In its 2009 report, “Range of Practices and Issues in Economic\nCapital Frameworks,” the Basel Committee on Banking Supervision recognizes that so far proposed statistical tests\nof portfolio credit models have weak power. “… The validation of economic capital models is at a very preliminary\nstage. … The validation techniques are powerful in some areas such as risk sensitivity but not in other areas such as\noverall absolute accuracy or accuracy in the tail of the loss distribution.”\nWhile these approaches provide insights into credit portfolio model validation, little is known empirically about the\nperformance of a credit portfolio model. Given the challenges, we proceed by focusing on the parts of the\ndistribution for which we have data. We illustrate a validation approach using historical corporate default experience\ndating back to 1983. This extensive data set is comprised of the defaults associated the firms rated by Moody’s\nInvestors Service (MIS) and the defaults associated with public firms collected by Moody’s Analytics (MA).2 We\nconstruct predicted default distributions using different types of PD and correlation inputs, and we then examine\nhow the predicted distribution compares with the realized distribution. We compare by looking at the percentiles of\nrealized defaults with respect to the predicted default distributions. We next look at the performance of two typical\nportfolio parameterizations: (1) a through-the-cycle style parameterization using agency ratings-based long-term\naverage default rates, and Basel II correlations, and (2) a point-in-time style parameterization using public EDF\ncredit measure, and Moody’s Analytics Global Correlation Model (GCorr).\nThe remainder of this paper is organized as follows:\n Section 2 presents the validation procedure.\n Section 3 describes the data for portfolio construction and alternative PD and correlation inputs.\n1 The two-state credit risk model neglects migration risk and assume zero recovery for all loans. As a consequence, the loss\ndistribution is fully described by the distribution of the number of defaults within a portfolio. The multi-state model incorporates\nboth migration risk and recovery rate uncertainty.\n2 For Moody’s definition of default, see “Corporate Default and Recovery Rates, 1920−2010” (2011). Section 4 discusses a comparison of realized default rates and predicted default rates under two test\nscenarios.\n Section 5 concludes.\nOur procedure begins with a portfolio of credit exposures of N companies active during the beginning of a year. A\ncredit portfolio model parameterized with default probabilities and correlations implies a distribution of default\nrates. The corresponding percentile of realized defaults is determined with respect to the estimated distribution of\ndefault counts. We estimate a percentile for each year. If the predicted loss distribution is equal to the true one, the\nobtained percentiles should follow independent and identically distributed uniform distributions.3\nRosenblatt (1952) defines the transformation\n∫ ̂( ) ̂( ) (1)\nWhere is the ex post portfolio profit/loss realization and ̂( ) is the ex ante forecast loss density. Rosenblatt shows\nthat when applying the estimate cumulative distribution function ̂( ) to observed losses, if the estimated loss\ndistribution is equal to the true one, the transformed variable follows i.i.d. U(0,1).\nThe detailed procedures follow. We create two sets of equally-weighted, vanilla term loan portfolios consisting of\n(1) all the MIS rated corporate firms and (2) all the non-financial public firms active in January of the test years. The\nnumber of active firms differs across time, so that the composition of the yearly portfolios varies over time for both\nportfolios. For each name, depending upon the test scenario, we assign a PD value upon either a rating-mapped PD\nor an EDF credit measure, and we then assign a GCORR correlation or asset correlation used by the Basel II\nIRB.4All loans have a 0% annual fixed rate coupon LGD of 100% and a one year maturity.\nWe analyze portfolios in Moody’s Analytics RiskFrontier™ to obtain portfolio value distributions at a one year\nhorizon using one million simulation trials. Because the LGD is set to 100% and no coupons are accumulated before\nhorizon, the portfolio value distribution is then converted to a distribution of realized default. For example, suppose\na portfolio has 100 instruments at the beginning of the year, each with a $1 million of commitment amount, i.e.,\n$100 million in total. With zero defaults, the portfolio value should remain $100 million. If, on a particular trial, the\nportfolio value is $95 million, the associated loss is $5 million and the number of defaulted obligors is five in this\ntrial. Similarly, one can calculate the number of defaults in other trials and tabulate results from each trial to\nconstruct a default distribution.\nWe next determine which percentile of the predicted distribution is associated with the realized default. Suppose\nduring a given year, the actual number of defaults is 10, which lies at the 95th percentile of the estimated default\ndistribution mentioned above. We record a 95th percentile value for that year. Repeating this exercise for the\nremaining years, we obtain a time series of mapped percentiles. If the model is correct, every percentile value should\noccur in [0,100] with the same probability. In other words, the percentiles should be independent and identically\nuniformly distributed. In addition, the frequency of exceptions (the number of actual defaults that exceed the number\nof predicted defaults indicated by VaR estimate) should be in-line with the selected target level of confidence. For\nexample, for VaR calculated at 99% confidence interval, exceptions can be expected every 100 mapped percentiles.\nThe following figure provides a visual representation of the mapping process. Each dot “•” in the right hand plot\nrepresents the mapped percentile of realized defaults with respect to the model-implied distribution at the specified\nyear. The model-implied distribution varies each year due to the change in portfolio composition and the changes in\nPD and correlation.\n3 See Rosenblatt (1952) and Berkowitz (2001) for details of the Rosenblatt transformation.\n4 The brief description of these two correlation measures can be found in Section 3.1\nRealized Defaults\n100\np90\ny c n\ne\ne litn\nu e\nq c\ne\nrF\nre\nP p50\nY3 Y1 Y2\np10\np10 p50 p90\n0\nPredicted Defaults Y1 Y2 Y3 … Year\nPredicted default distribution Percentiles of realized defaults with\nrespect to predicted default distribution\nFigure 2 through Figure 5 show examples of inaccurate models of various kinds. In Figure 2, the 2percentiles are near\nthe top end of the range, suggesting that the model underestimates the probabilities of defaults so that the realized\ndefaults largely fall into the tail region of the estimated distribution. On the contrary, if the model overestimates the\nprobabilities of defaults, the realized defaults tend to concentrate on the low end of the estimated default\ndistribution. The level of mapped percentiles becomes low, as shown in Figure 3.\n100\ny\nc\nn\ne u\nq\ne rF\ne litn\ne\nc\nThe average percentile is too high.\nre\nP\nRealized Defaults\n0\n3\nY1 Y2 Y3 … Year\nPredicted default distribution\ny 100\nc\nn\ne\nu e\nq\ne rF\nlitn\ne The average percentile is too low.\nc\nre\nRealized Defaults P\n0\nY1 Y2 Y3 … Year\nPredicted default distributionWe now explore the impact of correlations upon the distribution of percentiles under the assumption that the PD\nmodel is correct, and that the average PD is consistent with the average realized default rate. In Figure 4, we see that\nthe percentile values are widely scattered, located around either the top or the bottom of the range. The large\ndispersion of dots indicates that the model underestimates the correlation between obligors, as the model fails to\ncapture the extreme co-movements corresponding to joint defaults of underlying credits. Figure 5 shows the\nopposite scenario. The model’s correlation overestimation results in realized values falling within a narrow range\naround 50 percent. The dispersion of percentile values is limited.\n5\ny 100\nc\nn\ne The dispersion of percentile values\nu q\ne\nrF\ne litn\ne\nis too small.\nc\nre\nP\nRealized Defaults\n0\nY1 Y2 Y3 … Year\nPredicted default distribution\nThe following plot shows an example of a good model. On average, the realized defaults lie around the 50th\npercentile of the predicted default distribution. With a 10% of VaR level, 90 percent of dots are expected to lie\nbetween the 5th and 95th percentiles.6\n100\nAverage realized defaults should be\naround 50thpercentile\ne\nlitn\ne\nc\nre\nP\n90% of dots should lie between 5thand\n95thpercentile\n0\nY1 Y2 Y3 Y4 … … Yn Year\nThere are a variety of statistical tests available for accessing the adequacy of VaR measures. Kupiec’s (1995)\nproportion of failures Test (POF test) is a well known VaR back testing procedure. It examines whether the\nfrequency of exceptions is in line with the selected target level (unconditional coverage property).5 The exception is\ndefined as the case where the number of actual defaults exceeds the number of predicted defaults indicated by VaR\nestimate. If the number of exceptions is less than what the selected confidence level would indicate, the model\noverestimates risk. On the contrary, too many exceptions signal underestimation of risk. A shortcoming of the POF\ntest is that it does not examine the extent to which the independence property is satisfied. An accurate VaR measure\nshould exhibit both the independence and unconditional coverage property. Christoffersen’s (1998) proposed\nMarkov tests examine whether or not the likelihood of a VaR violation depends upon whether or not the violation\noccurred during the previous period. Christoffersen and Pelletier further suggest a duration test (2004) using the\ninsight that the time between VaR violations should not exhibit any kind of time dependence. Crnkovic and\nDrachman (1997) suggest the test on unconditional coverage and independence property to multiple VaR levels\ninstead of a single VaR level, to take the magnitude of the violations into account. Finally, Lopez (1999b) suggested\nloss function-based approaches.\nUnfortunately, the above tests have weak statistical power when using small datasets. Take the POF test as an\nexample. Violations occur rarely (the target probability is usually set small), and therefore testing whether violations\nform a Bernoulli requires a large sample size. Although a variety of statistic tests exist, these tests are data-intensive\nand not practical for validating credit portfolio models that describe extreme losses.\nIn this study, we use two default databases: defaults included in the Moody’s Investors Service (MIS) and in the\nMoody’s Analytics (MA) default database, collected and updated from numerous print and online sources\nworldwide. For the MIS default database, we focus on defaults associated with Moody’s-rated corporate issuers,\ngiven that rated defaults are better documented than unrated defaults. Moody’s uses the senior ratings algorithm\n5 See Kupiec (1995) for the description of the test.(SRA) to derive issuer-level ratings and are referred to as estimated senior ratings.6 We employ data after 1983 in\norder to make sure the rated firms have consistent ratings during the sample period. These ratings are used as one of\nthe PD inputs in our tests. The second data source, MA’s default database of non-financial public companies 1980\nand 2010, is the most extensive public company default database available.\nThe PD inputs for public companies come from Moody’s Analytics EDFTM (Expected Default Frequency) credit\nmeasures − probabilities of default for firms with publicly traded equity and published financial statements derived\nusing the Vasicek-Kealhofer (VK) model. This model provides a rich framework that treats equity as a perpetual\ndown-and-out option on the underlying assets of the firm. This framework incorporates five different classes of\nliabilities: short-term liabilities, long-term liabilities, convertible debt, preferred shares, and common shares. The\nVK model uses an empirical mapping based on actual default data to get the default probabilities, known as EDF\ncredit measures. Volatility is estimated through a Bayesian approach that combines a comparables analysis with an\niterative approach. For an overview of the EDF credit measure, see Crosbie and Bohn (2003).\nThe following figures show the number of unique firms and the associated defaults by year for the MIS and MA\ndatabases, respectively. Both datasets provide global coverage.\nNumber of Rated Firms and Their Defaults by Year (MIS)\n6000 300\n5000 250\nsm\nstlu\nriF\nfo\nre\nb\nm34 00 00 00 12 50 00a fe\nD\nfo\nre\nb\nu m\nn e\nh\nT2000 100u n\ne\nh\nT\n1000 50\n0 0\n34567890123456789012345678901\n88888889999999999000000000011\n99999999999999999000000000000\n11111111111111111222222222222\nthe number of firms the number of defaults\n6 Before 1983, the broad rating category is: Aaa, Aa, A, Baa, Ba, B, Caa, Ca, C. Since April 1982, numerical modifiers (using the\n1, 2, 3 modifiers) were appended to each generic rating classification from Aa through Caa. See “Moody’s Senior Ratings\nAlgorithm & Estimated Senior Ratings” (2009) for details.Number of Non-financial Public Firms and Their defaults\nby Year (MA)\n35000 700\n30000 600\nsm25000 500\nstlu\nriF\nfo\nre\nb\nm12 50 00 00 00 34 00 00\na fe\nD\nfo\nre\nb\nu m\nn\ne h T10000 200\nu\nn e\nh\nT\n5000 100\n0 0\n01234567890123456789012345678901\n88888888889999999999000000000011\n99999999999999999999000000000000\n11111111111111111111222222222222\nthe number of firms the number of defaults\nNote the default databases from MIS and MA used here have different data coverage. If a company has debt rated by\nMIS but does not have public equity, then its default is not recorded in the MA database.\nWe use two types of correlation measures.7 The first one is defined by the Basel II IRB formula as follows.\n- (- ) - (- )\n( ) ( - ) (2)\n- (- ) - (- )\nParameters a, b, and c depend upon borrower type. For corporate borrowers, a=0.12, b=0.24, and c=50. The above\nexpression indicates that the asset correlation parameter R is a decreasing function of PD.8\nThe alternative correlation measure is Moody’s Analytics global asset correlation (GCorr), a multi-factor model\nestimated from weekly asset return series. The asset returns are derived from equity returns and liability structure\ninformation using an option-theoretic framework. The GCorr model has broad data coverage. For example, it covers\nmore than 34,000 firms in 49 countries and 61 industries, January 2008 through June 2009. First released in 1996,\nthe GCorr model is updated on a regular basis to reflect the most recent dynamics of firms’ businesses and\nindustries.\nOne common default data collection challenge remains the fact that small public companies often disappear without\nany news or record before they default, or they do not publicly disclose missed payments, both of which creates a\nnumber of hidden defaults. To alleviate this hidden defaults problem, in the second case, we restrict the sample to\nfirms above at least $300 million in annual sales, where we believe hidden defaults are less of an issue. We discuss\nthe hidden default issue further in Section 4.2.\n7\nFor details of the Basel II correlation, see the Basel Committee on Banking Supervision, “International Convergence on Capital\nMeasurement and Capital Standards,” (2006). For the GCorr model, see Moody’s Analytics document “An Overview of\nModeling Credit Portfolios,” (2008).\n8\nSee the Basel Committee on Banking Supervision, (2006), “International Convergence on Capital Measurement and Capital\nStandards.”We create two portfolios for test purposes. Portfolio-specific settings are highlighted in the following input table in\norder to make a distinction.\nWe choose these cases as they represent common parameterization approaches by risk management groups at\nfinancial institutions. Case 1 represents a regulatory-style parameterization where Basel correlations are used, and\ndefault probabilities are based upon a measure frequently associated with a through-the-cycle concept. Meanwhile,\nCase 2 represents a more point-in-time measure, where both PDs and correlations are parameterized using forward\nlooking measures.\nOne may notice that if the above two approaches are applied to common datasets the results would be more directly\ncomparable. However, the limited data of public companies with rating histories in our database precludes such an\nattempt.\nIn Case 1, we use the long term average of the actual default rate by rating as the PD estimate. As a result, firms\nwith same rating are assigned the same PD value regardless of the sample year. The long term average PD is free of\nthe credit cycle effect and reflects the long-run credit risk level. Table 2 shows the mapping from the rating to long-\nterm average default rate.9\n9 The long-term average default probability is calculated from raw empirical data and may not increase monotonically as the\nrating drops.The following figure shows the portfolio composition by letter rating. The composition is relatively stable over time.\n“Aaa” through “A” rated names account for the largest portion of the whole portfolio most of the time. The “Caa”\nthrough “C” rated names represent the smallest portion of the entire portfolio.\nPortfolio Composition by Year\n100%\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%\n4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 1 1\n9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\nAaa-A Baa Ba B Caa-C\nAs mentioned previously, we use the long-term average of the realized default rate by rating category as the PD\nestimate for each name in the portfolio in a given year. Equation (1) is then parameterized with this PD to calculate\nthe corresponding correlation defined in Basel II IRB. Figure 10 shows the average PD and Basel RSQ of portfolios\nby year.\nAverage PD and RSQ of Portfolios by Year\n0.22 0.025\n0.215\n0.02\n0.21\n0.205\n0.015\nQ SR 0.2 D P\n0.01\n0.195\n0.19\n0.005\n0.185\n0.18 0\n3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 1 1\n9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\nmean RSQ mean PDIn Case 2, we construct yearly US portfolios by pooling all US non-financial firms available at the beginning of the\nyear beginning in 1980. We use the MA EDF credit measure as the PD measure and GCorr as the correlation input.\nGCorr is a multifactor model that assumes co-movements among asset returns are driven by a set of common\nfactors. Unlike historical correlations containing random noise, in addition to useful information, a well-constructed\nmultifactor model produces more accurate forward-looking correlation measures. The GCorr model is updated\nregularly. Specifically, GCorr1996, GCorr1999, GCorr2002, GCorr2003, GCorr2004, GCorr2005, GCorr2006,\nGCorr2007, GCorr2009, and GCorr2010 have been released since 1996. In constructing the portfolio, we use the\nproper version of the GCorr model as of the test year. For portfolios earlier than 1996, we use the modeled R-\nsquared calculator for GCorr2002 to calculate modeled RSQ, as it is the earliest available version. The Modeled R-\nsquared Calculator is a non-linear econometric model of three underlying factors: country weight, industry weights,\nand firm size.10\nFigures 11 and 12 show the number of firms in the US portfolios.\nNumber of Public Firms and Their Defaults by Year\n(Large US Non-financial)\n2500 120\n100\n2000\nsm\nrif\nfo1500\n80 s\ntlu\nre\nb m\nu1000\n60 a fe\nd\nfo\nn\ne\n40 #\nh\nT\n500\n20\n0 0\n01234567890123456789012345678901\n88888888889999999999000000000011\n99999999999999999999000000000000\n11111111111111111111222222222222\nthe number of firms the number of defaults\nFigure 12 shows the average EDF and the GCorr RSQ.\n10 See Moody’s Analytics white paper “Moody’s KMV Private Firm R-squared Model” for details.We analyze the yearly portfolios above in RiskFrontier in order to obtain the estimated portfolio value/default\ndistribution at the one year horizon. We then compute the percentiles of realized defaults. Note that the estimated\ndistribution may not be smooth. For example, in 1983, there are 1,330 firms and 11 realized defaults in the sample\nportfolio, while in the estimated portfolio distribution, the number of 11 defaults corresponds to a range of\npercentiles from the 60th percentile to the 64th percentile. Under this circumstance, we take an average of 60 and 64\nand use the mean value of 62 as the percentile for the default realization.\nWe now look at through-the-cycle (Case 1). In Figure 13, each diamond “♦”represents the assigned percentile after\nmapping the realized number of default to the estimated default distribution at the corresponding year under the\nCase 1 parameterization. The plot demonstrates a large swing in the percentile points and a strong serial correlation\nbetween percentiles.Percentile of Realized Default wrt Predicted Default\n(Long-term average DR, Basel RSQ)\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 1 1\n9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\nIn Figure 14, by looking at the autocorrelation function (ACF) plot of the differenced percentile values with a 95%\nconfidence band, we can identify that the autocorrelation is significant at lag 1. The autocorrelation indicates that the\nmodel tends to consecutively either overestimate or underestimate the number of defaults. This finding is\nunderstandable given that a through-the-cycle (TTC) style PD dampens the sensitivity of the PD to the\nmacroeconomic conditions and does not reflect the improvement or deterioration credit quality, causing a\nconsequent overestimation or underestimation of portfolio losses.Using percentile values, we conduct a two-sided Kolmogorov-Smirnov Goodness-of-Fit test for uniform distribution\n[0,100] and obtain a p-value of 0.0582. The test statistic falls into the neighboring area of the rejection region if we\nspecify the null hypothesis that the true distribution is a uniform distribution with a significance level of 0.05.\nWe now look at point-in-time (Case 2). In contrast to Figure 13, Figure 15 shows that the percentile points are not\nserially correlated over time under the Case 2 parameterization. Rather, the percentiles are distributed relatively\nevenly in the interval of [0, 100], indicating a more accurate assessment of credit risk on the portfolio level. We also\nsee that EDF levels are consistently conservative (i.e. high relative to realized defaults), and the percentile points\nremain below the 65th percentile of estimated distribution.\nPercentile of Realized Default wrt Predicted Default\n(US Large Non-financial, EDF, GCorr)\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 1 1\n9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\nIt has been documented that the EDF credit measure is conservative (i.e., higher than observed default rates even at\ndifferent annual sales restrictions) due to the hidden defaults issue.11 This failure to capture all defaults can occur for\nvarious reasons. For example, when a debt extension occurs, it is difficult for an outsider to know if the extension is\ncaused by the borrower’s inability to pay or by legitimate business need. In other cases, when the loan amount is\nsmall, failure to pay is simply written off by the bank, and no public announcement is released. When default data\ncollection relies upon public information to identify defaults, many default events may go missing. This scenario is\nparticularly true for smaller firm borrowers whom draw little public attention.\nMoody’s Analytics’ team of specialists aggregates default data utilizing multiple information sources including, but\nnot limited to: bankruptcy newsletters, rating agency debt monitoring publications, news media and news search\nengines, corporate regulatory filings, internet browsing, and targeted searching. Despite being the largest public\ndefault database we are aware of, it is possible that a significant number of defaults are not captured in the data,\nbecause in many cases distressed borrowers work out deals privately with lenders, drawing little attention in the\nmedia or by data collectors. As there is generally less information available for smaller companies, we believe the\nhidden default problem is larger for smaller companies. In calibrating the EDF model, we consciously employ only\n11 See Crossen, Qu, and Zhang (2011), “Validating the Public EDF Model for North American Corporate Firms.”large companies in mapping distance-to default to EDF levels in order to circumvent this problem. In addition, the\nmapping is constructed so the EDF measure is conservative relative to the long-term average default rate, even in the\nlarge company sample.\nMeanwhile, the EDF credit measures are not too conservative, as we do not observe any extremely low percentiles\nnear the 0th percentile (i.e., over-estimation of the number of defaults) in Figure 15.\nGiven the non-linear relationship between PD and correlation, we scale the EDF measures to match the average\nrealized default rate during the period 1980−2010, while keeping the GCorr correlation unchanged. We then rerun\nthe portfolios with updated PD and correlation inputs. Figure 16 shows the result. Percentiles are less extreme on the\nlow end of the interval.\nPercentile of Realized Default wrt Predicted Default\n(US Large Non-financial, Scaled EDF, GCorr)\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 1 1\n9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\nAfter matching EDF measures and realized default rate in level, we can focus on examining the effect of correlation\nupon the distribution of percentile. By visual inspection, we see the moderate level of dispersion among percentiles\nwith the points ranging from the 25th to 85th percentile, and the percentile points are still not serially correlated over\ntime. This randomness is confirmed by computing autocorrelations for percentile values at varying time lags. In\nFigure 17, none of the autocorrelations at different time lags (lag1 to lag25) significantly differs from zero.Using the percentile values in Figure 16, we also conduct a two-sided Kolmogorov-Smirnov Goodness-of-Fit test\nfor uniform distribution [0,100] and obtain a p-value of 0.047, which rejects the null hypothesis that the true\ndistribution is a uniform distribution with a significance level of 0.05. Given that the clustering of percentile points\nconcentrates around the mean, we explore the uniform range of [10, 90] by repeating the test for uniform\ndistribution [10, 90] and obtain a p-value of 0.233, which fails to reject the null hypothesis at a 5% of significance\nlevel.\nThe evaluation of credit portfolio risk model is an important topic. The recent discussion on practices and issues in\neconomic capital frameworks by the Basel Committee highlights this fact. An approach explored by researchers\ninvestigates whether or not statistical tests used in market risk can be transferred to evaluate credit risk. The\napplicability of such an approach for the validation of credit risk models is limited due to the limited number of\nhistorical observations available in credit risk.\nIn this study, we illustrate a validation approach and provide empirical evidence on the ability of credit portfolio\nmodel in describing the distribution of portfolio losses. We construct yearly portfolios based upon two typical\nportfolio parameterizations: (1) a through-the-cycle style parameterization using agency ratings-based long-term\naverage default rates and Basel II correlations and (2) a point-in-time style parameterization using public EDF credit\nmeasures and Moody’s Analytics Global Correlation Model (GCorr).We then compare the percentiles of realized\ndefaults with respect to the predicted default distributions under different settings.\nResults demonstrate that the through-the-cycle style parameterization results in a less conservative view of\neconomic capital and substantial autocorrelation in capital estimates. Results also demonstrate that point-in-time\nmeasures help produce consistent and conservative estimates of economic capital over time.Basel Committee on Banking Supervision (2009). Range of practices and\nissues in economic capital frameworks.\nBasel Committee on Banking Supervision (2006). International\nconvergence on capital measurement and capital standards.\nBerkowitz, J. (2001). Testing density forecasts, with applications to risk\nmanagement. Journal of Business and Economic Statistics 19, 465-\n474.\nCrosbie, P., and Bohn, J. (2003). Modeling default risk. Moody’s\nAnalytics Technical Document.\nCrossen, C., Qu, S., and Zhang, X. (2011). Validating the public EDF\nmodel for North American corporate firms. Moody’s Analytics White\nPaper.\nCrossen, C., and Zhang, X. (2011).Validating the public EDF Model for\nglobal financial firms. Moody’s Analytics White Paper.\nFrerichs, H., and Löffler, G. (2003). Evaluating credit risk models using\nloss density forecasts. Journal of Risk 5, 4, 1–23.\nInanoglu, H., and Jacobs, M. (2010). Models for risk aggregation and\nsensitivity analysis: an application to bank economic capital, The\nJournal of Risk and Financial Management, Vol. 2 (Summer)\nJacobs, M. (2010). Validation of economic capital models: state of the\npractice, supervisory expectations and results from a bank study,\nJournal of Risk Management in Financial Institutions, Vol. 3, 4 334–\n365.\nKupiec, P. (1995). Techniques for verifying the accuracy of risk\nmanagement models. Journal of Derivatives 4, 3, 73–84.\nLevy, A. (2008). An overview of modeling credit portfolios. Moody’s\nAnalytics White Paper.\nLopez, J. A., and Saidenberg, M.R. (2000). Evaluating credit risk models.\nJournal of Banking & Finance 24, 151–165.\nMoody’s Investor Services, 2011. Corporate default and recovery rates,\n1920−2010. Special Comment.\nMoody’s Investor Services, 2009. Moody’s senior ratings algorithm &\nestimated senior ratings. Special Comment.\nRosenblatt, M. (1952). Remarks on a multivariate transform. The Annals\nof Mathematical Statistics 23, 470–472."
}